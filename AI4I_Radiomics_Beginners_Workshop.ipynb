{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiGMHG8iYoWV"
   },
   "source": [
    "# Radiomics for beginners\n",
    "  \n",
    "<b>'Data curation is the most laborious and boring step of the radiomics workflow' - anyone who has ever worked with radiomics</b>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this script, you will use an open-source dataset of 422 NSCLC cancer patients named NSCLC Radiomics in TCIA https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics, also known as MAASTRO-Lung-1 (Aerts, H. J. W. L., Wee, L., Rios Velazquez, E., Leijenaar, R. T. H., Parmar, C., Grossmann, P., Carvalho, S., Bussink, J., Monshouwer, R., Haibe-Kains, B., Rietveld, D., Hoebers, F., Rietbergen, M. M., Leemans, C. R., Dekker, A., Quackenbush, J., Gillies, R. J., Lambin, P. (2019). Data From NSCLC-Radiomics [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2015.PF0M9REI), Attribution-NonCommercial 3.0 Unported license (https://creativecommons.org/licenses/by-nc/3.0/). You will build a classifier to differentiate contast-enhanced from non-contrast-enhanced lung CT scans using radiomic features extracted from the gross tumor volume (GTV). Incredibly, these data were originally (and still are) used without this differentiation, leading to poor model performance for clinical outcomes. Such classifying models can easily help you curate your dataset or add a new label to your list of predictors.\n",
    "\n",
    "<b>Brief description</b>\n",
    "\n",
    "Iodinated contrast agents can be used for obtaining a CT image depending on the clinical question that one wants to address. The purpose of such agents can be to enhance the contrast resolution between a lesion/ischemic or occluded region and the normally perfused surrounding tissues.\n",
    "Sometimes contrast-enhanced CT is desireable, though the patient has an absolute contra-indication for contrast, such as:\n",
    "* allergy to intravenous contrast media,\n",
    "* pregnancy (particularly in the first trimester),\n",
    "* severe renal impairment,\n",
    "* hyperthyroidism/goitre (IV contrast might induce thyrotoxic crisis), etc.    \n",
    "\n",
    "The task of assessing whether an image is contrast-enhanced is <b>fairly easy in early/late arterial phases</b> (major arteries appear as hyperdense, white structures immediately after contrast bolus is administered through a peripheral vein), but may become <b>more challenging in the late portal phase</b> (blood supply from arteries reaches the liver and from there goes back into the venous system through the portal vein) and delayed venous phases (most contrast washes out from the tissue and is filtered through the kidneys).<br>  \n",
    "\n",
    "| ![picture](https://drive.google.com/uc?export=view&id=1Kw47_0GUdvFwPjvjcGW588SGMXuV9TpF)      | ![picture](https://drive.google.com/uc?export=view&id=1w288Lda5tI6_4sy3sFepgGX4HB0YYm-z)                                                                                         |\n",
    "|:-----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "| Patient without contrast-enhancement and with a mass in the right upper lobe<br>(red circle) | Patient with contrast enhancement (upper red circle); <br>both the major chambers/arteries of the heart as well as the aorta (arrow) <br>appear as hyperdense white structures |\n",
    "\n",
    "<b>Problem</b>  \n",
    "\n",
    "Usually information about the contrast is <b>not included in the DICOM tags</b>, and manual assesment of the CT for the purpose of the data curation can take considerable time. In this task, we are adressing this problem by using radiomics of the GTV to classify the images. The ground truth labelling was performed by a radiology expert.\n",
    "\n",
    "<b>Notebook structure</b>  \n",
    "\n",
    "This Python script will take you through the radiomics analysis steps where you'll learn how to read in the features, select the features, train the models and display the results:\n",
    " - installing packages and importing libraries,  \n",
    " - preparation of the imaging data,\n",
    " - features extraction,\n",
    " - reading the data and assigning outcomes,\n",
    " - exploring the data,\n",
    " - removing highly correlated features,\n",
    " - selecting features using recursive feature elimination,\n",
    " - creating the classification models, displaying the results.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "This is an interactive Python notebook. To run it, you don't need to install anything on your PC since the script is executed in the cloud. On the left tab, you can see the 'Files' folder, that contains the data. Results of the script execution will be saved in this folder.  \n",
    "  \n",
    "The dataset is open-source, so you can download the data to have a look at it. It you don't have any software to open DICOM files, you can download and install 3D Slicer (https://www.slicer.org/), RadiAnt viewer (https://www.radiantviewer.com/), or MicroDicom viewer (https://www.microdicom.com/downloads.html). As feature extraction process takes time, we prepared the .csv tables with the pre-extracted features to work with. They are available at the same shared folder. All the data will be pulled from the shared Google Drive to the temporary environment of this notebook. Further it is explained how to get these data.  \n",
    "\n",
    "Please note, that all the files you pull or upload to this notebook, as well as the files, produced while executing the script, are automatically deleted as soon as you end the session.\n",
    "\n",
    "First of all, the needed Python packages have to be uploaded. Some of them are not installed in the environment of this notebook, so the installation is needed with '!pip install <i>name-of-the-package</i>' command. We recommend you get acquainted with getting documentation and help on these packages. For example, google 'python sklearn' and you will get to the documentation quickly. Importing libraries is a necessary step with most progamming languages, not only Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16694,
     "status": "ok",
     "timestamp": 1687867628496,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "60z4gB0WdAnj",
    "outputId": "b4a33852-f9bc-4668-bfd2-92551a3f2c41"
   },
   "outputs": [],
   "source": [
    "# installing some packages, which are not part of the present Google Collab environment\n",
    "\n",
    "!pip install precision-medicine-toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwuI_iH4UELF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmtool.AnalysisBox import AnalysisBox\n",
    "from pmtool.ToolBox import ToolBox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16449,
     "status": "ok",
     "timestamp": 1687866610250,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "bZ7ovtxd9wXj",
    "outputId": "bf5a83ba-75d1-4b74-fab5-4c5fa3674515"
   },
   "outputs": [],
   "source": [
    "!gdown --id 15CPMLABDUtDpsnoFBqSu5P4d8b7tBiod\n",
    "!unzip AI4I2022_Radiomics_Beginners_Data_sample15.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzuYSO_Nmn43"
   },
   "source": [
    "## Feature extraction\n",
    "There are many softwares to perform this step, each with their strengths and weaknesses. Features extraction step takes the longest time, so we are happy to help you with it. We will supply you with \"something we prepared earlier\", the lists of features already extracted with Radiomics software, and outcomes (CE/non-CE). The data is already split into train and validation sets. Therefore, you can skip this step or return to it later. If you want to skip this step, please move to 'Reading the data' step.  \n",
    "\n",
    "If you want to extract the features by yourself anyway, you can execute the following code. Here we perform features extraction with PyRadiomics software (https://pyradiomics.readthedocs.io/en/latest/) with the precision-medicine-toolbox (https://github.com/primakov/precision-medicine-toolbox) interface. These are the open-source Python packages.  \n",
    "\n",
    "First of all, we would need to convert DICOM data into NRRD. But we will start with unzipping the imaging archive. Pulling the shared files and unzipping (pay attention at 'Files' tab of this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 12772,
     "status": "ok",
     "timestamp": 1687867797178,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "i-ef2X2zutUd",
    "outputId": "c692f8ca-7d64-432e-a89d-7a92d1b1170f"
   },
   "outputs": [],
   "source": [
    "# listing the DICOM dataset parameters:\n",
    "parameters = {'data_path': '/content/AI4I2022_Radiomics_Beginners_Data_sample15/data_dicom', # path to your DICOM data\n",
    "              'data_type': 'dcm', # original data format: DICOM\n",
    "              'multi_rts_per_pat': False}   # when False, it will look only\n",
    "                                            # for 1 rtstruct in the patient folder,\n",
    "                                            # this will speed up the process,\n",
    "                                            # if you have more then 1 rtstruct per patient,\n",
    "                                            # set it to True\n",
    "\n",
    "# initializing the dataset object:\n",
    "data_dcms = ToolBox(**parameters)\n",
    "\n",
    "# getting the description for the first 10 files:\n",
    "dataset_description = data_dcms.get_dataset_description('CT')\n",
    "dataset_description.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh6Br3hUu-2D"
   },
   "source": [
    "We can plot some scanning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2412,
     "status": "ok",
     "timestamp": 1687771296451,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "uwAbsibOvChZ",
    "outputId": "5d4ba60d-1a4f-4e9b-b0f6-bfb8614137d7"
   },
   "outputs": [],
   "source": [
    "sns.set(context='poster', style='whitegrid')\n",
    "\n",
    "study_date = sorted([ 'Nan' if x=='' or x=='NaN' else str(x[0:4]) for x in list(dataset_description['SeriesDate'])])[2:]\n",
    "conv_kernel =['Nan' if x=='' or x=='NaN' else x for x in list(dataset_description['ConvolutionKernel'])]\n",
    "tube_current =[-1 if x=='' or x=='NaN' else x for x in list(dataset_description['XRayTubeCurrent'])]\n",
    "exposure =[-1 if x=='' or x=='NaN' else x for x in list(dataset_description['Exposure'])]\n",
    "ps = sorted([(x[0]) for x in list(filter(lambda x: x != 'NaN', dataset_description['PixelSpacing'].values))])\n",
    "sl_th = sorted([str(x)[0:3] for x in list(filter(lambda x: x != 'NaN', dataset_description['SliceThickness'].values))])\n",
    "figures,descriptions = [study_date,conv_kernel,tube_current,exposure,ps,sl_th],['Study Date','Conv Kernel','Tube Current','Exposure','Pixel spacing','Slice Thickness']\n",
    "\n",
    "fig,ax = plt.subplots(2,3,figsize=(25,15))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax[i,j].hist(figures.pop(0),alpha=0.7)\n",
    "        ax[i,j].set_title(descriptions.pop(0),fontsize=20)\n",
    "\n",
    "plt.suptitle('Imaging parameters in the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC9FV4BbvEyz"
   },
   "source": [
    "As PyRadiomics do not support DICOM, we convert images and masks into NRRD format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37289,
     "status": "ok",
     "timestamp": 1687771351126,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "L2R4AE55vJPz",
    "outputId": "9afe84d7-1dc4-441a-8891-46d00403303f"
   },
   "outputs": [],
   "source": [
    "data_dcms.convert_to_nrrd('/content/', 'gtv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqtCfgnnvLOi"
   },
   "source": [
    "The data is saved in temporary folder and will be lost as soon as you reboot the notebook! We can read the NRRD data and get sure the convertion was correct and the images are co-aligned with the masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300128,
     "status": "ok",
     "timestamp": 1687868109330,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "Pi9RRBx2vQY6",
    "outputId": "eed3a777-5c03-4edf-b113-0194c6698934"
   },
   "outputs": [],
   "source": [
    "# initializing the newly created NRRD dataset:\n",
    "data_nrrds = ToolBox(data_path = '/content/converted_nrrds',\n",
    "                     data_type='nrrd')\n",
    "\n",
    "# saving snapshots to JPEG files\n",
    "data_nrrds.get_jpegs('/content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG4vjCm4vSgu"
   },
   "source": [
    "A quick look at the contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AQ-lW1JvXm-"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def browse_images(images,names):\n",
    "    n = len(images)\n",
    "    def view_image(i):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.imshow(images[i])#, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.title('Slice: %s' % names[i])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    interact(view_image, i=(0,n-1))\n",
    "\n",
    "for pat,_ in data_nrrds:\n",
    "    _,file_struct = [*os.walk(os.path.join('/content/images_quick_check/',pat))]\n",
    "    root,images = file_struct[0],file_struct[2]\n",
    "    imgs =[np.array(Image.open(os.path.join(root,img))) for img in images]\n",
    "    print(pat)\n",
    "    browse_images(imgs,images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTDiudPavalJ"
   },
   "source": [
    "In the precision-medicine-toolbox, we are using PyRadiomics software (https://pyradiomics.readthedocs.io/en/latest/) to extract the features. You can read the full documentation for the currently stable version: https://pyradiomics.readthedocs.io/_/downloads/en/stable/pdf/.\n",
    "\n",
    "We are using PyRadiomics parameters file customized for CT data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1ajdi67vp5M"
   },
   "outputs": [],
   "source": [
    "parameters = \"/content/AI4I2022_Radiomics_Beginners_Data/example_ct_parameters.yaml\"\n",
    "features = data_nrrds.extract_features(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8VVpdSQvs3k"
   },
   "source": [
    "Let's have a look at the dataframe with he features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJncp1fkvv4R"
   },
   "outputs": [],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yja8CdFGvy4b"
   },
   "source": [
    "Now you know how to extract features. Nevertheless, in the prepared files, we also have an expert's outcome (CE/non-CE), therefore, from now on, we recommend to use those .csv files. Moreover, we selected only one ROI per patient (GTV) and the patients were split into training and testing sets. The key difference is that the present script is using Pyradiomics for feature extraction whereas in the prepared files, Radiomics software (https://radiomics.bio/) was used.\n",
    "\n",
    "## Reading the data\n",
    "\n",
    "For this step, you will need to specify the files containing the features and outcomes (in this case 0 or 1 for CE/non-CE). We have already cleaned the dataset to remove low quality CTs, and separated the data into training (N=253) and testing (N=106) groups.\n",
    "It is important that apart from initial dataset cleaning, all the work is only performed in the training dataset and the test set remains untouched.\n",
    "\n",
    "<b>NOTE: the separator character, text quotation, decimal point character change depending on your country settings. If this step fails, add those settings into read_csv function. Also, the outputs from different radiomics softwares can differ wildly, so some thought is necessary when doing this at home.</b>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1687866642946,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "EQ-YoYDCwJXo",
    "outputId": "ccaf172c-fa1b-478b-df9b-de05d8b75ff8"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"/content/AI4I2022_Radiomics_Beginners_Data_sample15/data_features/WS2022_Beginners_TrainingSet.csv\")\n",
    "data_test = pd.read_csv(\"/content/AI4I2022_Radiomics_Beginners_Data_sample15/data_features/WS2022_Beginners_ValidationSet.csv\")\n",
    "\n",
    "# let's have a look at our dataframe:\n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FoSexoRwOm8"
   },
   "source": [
    "<b>Getting the features and defining the outcome</b>\n",
    "\n",
    "It's a good time to look at the .csv files using Google Tables or similar software. IF the data looks jumbled up, try \"importing from text\". Ask a helper if you need assistance with this. Let's get the list of the features from the .csv header first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1687866649384,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "-NbbS-JbwePK",
    "outputId": "a9e799fe-40dc-489d-d5bb-54d3dfcac65f"
   },
   "outputs": [],
   "source": [
    "features = ...........# Print features names\n",
    "\n",
    "print (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLgipVwkwlj5"
   },
   "source": [
    "Separate the features and the outcomes:\n",
    "Create one datafranme for features and one for outcomes from features.csv above\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1687866651699,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "eRAc1kzqwoQ-",
    "outputId": "394d6670-644a-4048-ace0-25edcc0b3805"
   },
   "outputs": [],
   "source": [
    "outcome_train = ........\n",
    "outcome_test = ........\n",
    "data_train = ........\n",
    "data_test = .........\n",
    "\n",
    "# checking how the features dataframe looks like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBm8KYFpyZTW"
   },
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Before building the models, it is always useful to perform the exploratory data analysis to have a look at the data, understand the distribution of the features, and notice some data errors. To perform these steps, we will be using precision-medicine-toolbox (https://github.com/primakov/precision-medicine-toolbox), an open-source Python package, developed in the D-Lab, Maastricht University.  \n",
    "  \n",
    "Remember that for now, everything happens in the training data. Time to discuss why this is so important!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1687866667270,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "paW5Ay4lyhi4",
    "outputId": "7213d4df-3310-4650-9ff3-90a9f89838b6"
   },
   "outputs": [],
   "source": [
    "# let's list the parameters of our features dataset\n",
    "parameters = {\n",
    "    'feature_path': \"/content/AI4I2022_Radiomics_Beginners_Data_sample15/data_features/WS2022_Beginners_TrainingSet.csv\", # path to csv/xls file with features\n",
    "    'outcome_path': \"/content/AI4I2022_Radiomics_Beginners_Data_sample15/data_features/WS2022_Beginners_TrainingSet.csv\", #path to csv/xls file with outcome (in our case - the same file)\n",
    "    'patient_column': 'General_PatientID', # name of column with patient ID\n",
    "    'patient_in_outcome_column': 'General_PatientID', # name of column with patient ID in clinical data file\n",
    "    'outcome_column': 'Outcome' # name of outcome column\n",
    "}\n",
    "\n",
    "# initializing the dataset, containing features\n",
    "fs = AnalysisBox(**parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_EgaYg4ykq-"
   },
   "source": [
    "Plotting feature distributions for the first 80 features to have a visual understanding about feature distributions in classes. For this and the next steps, interactive .html reports will be generated. You can find them on your google Drive in the same folder as the original .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G37us_8FymlQ"
   },
   "outputs": [],
   "source": [
    "fs.plot_distribution(fs._......)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvfwDvlnyplK"
   },
   "source": [
    "Plotting the correlation matrix for the first 80 features to have an idea about how many features are inter-correlated. Why is it important to know about the mutual feature correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igN2PiRyyrNL"
   },
   "outputs": [],
   "source": [
    "fs............(fs._feature_column[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzLIHLwNytEG"
   },
   "source": [
    "Performing Mann-Whitney U-test (with False Discovery Rate correction) to see if the feature distributions in classes are statistically different (for the first 80 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbmFZWfTywa5"
   },
   "outputs": [],
   "source": [
    "..................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idLMYzbVyybP"
   },
   "source": [
    "\n",
    "Plotting univariate ROC curves and calculating AUC scores to have some estimation of the predictive power of every separate feature (for the first 80 features). Can we just build a classifer based on the best feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRfw0MIay2XS"
   },
   "outputs": [],
   "source": [
    "auc_th = 0.5 # Modify\n",
    "fs.plot_univariate_roc(............, auc_threshold=auc_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsX4cex6y4vp"
   },
   "source": [
    "Performing volumetric analysis to understand if our features are correlated to ROI volume and if the volume itself is a good predictor for our task. Why is it important to know if the features are correlated to volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gaI7isky6ou"
   },
   "outputs": [],
   "source": [
    "corr_th = 0.5 # Modify\n",
    "fs.volume_analysis(volume_feature='Shape_volume', corr_threshold=corr_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce691Exmy8oy"
   },
   "source": [
    "Writing the basic statistics for every feature into .csv file and having a look at the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "executionInfo": {
     "elapsed": 57872,
     "status": "ok",
     "timestamp": 1687866918194,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "rEjH5fW9zDaV",
    "outputId": "3147fa62-6a3d-416b-b9a9-4ee735789d3d"
   },
   "outputs": [],
   "source": [
    "fs.calculate_basic_stats(volume_feature='Shape_volume')\n",
    "\n",
    "print('Basic statistics for each feature')\n",
    "pd.read_excel('/content/AI4I2022_Radiomics_Beginners_Data_sample15/data_features/WS2022_Beginners_TrainingSet_basic_stats.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjxiXEuFzENo"
   },
   "source": [
    "## Features reduction and selection\n",
    "\n",
    "We will remove columns with NA values and remove features with (near)zero variance, as they do not have value for classification and needlessly increase computation and dimensionality.\n",
    "\n",
    "First, we will deal with the missing (NA) and zero variance values. Should we drop rows or columns of the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1687866918194,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "8kX0JElHzXn3",
    "outputId": "78646ed0-98dc-4df4-b9da-04de34c4638f"
   },
   "outputs": [],
   "source": [
    "features_non_nan = data_train.dropna(axis=1).columns\n",
    "print ('Number of features without missing values: ', len(..............))\n",
    "\n",
    "features_non_zero_var = data_train[features_non_nan].loc[:,\n",
    "                                                         data_train[features_non_nan].std() > 0.3].columns\n",
    "print ('Number of features with non-zero variance: ', len(..............))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5IUCHeFzak0"
   },
   "source": [
    "Removing highly correlated features is a controversial step aimed to reduce the dimensionality of the feature space. Highly correlated features needlessly inflate the dimensionality of feature space. The idea is that highly correlated features can be grouped together and represented by one representative feature.  For features pairs with a high Spearman correlation (r > 0.9) the feature with the highest mean correlation with all remaining features is removed.\n",
    "The opponents to this step argue that just because features are correlated doesn't mean that they don't individually  increase the model's performance.\n",
    "Time to discuss the pro's and con's of this step in more detail!\n",
    "\n",
    "<b>If you have time at the end of today's workshop, the cutoff is certaily a variable that can be played with.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6b4jVyizlb4"
   },
   "outputs": [],
   "source": [
    "def selectNonIntercorrelated(df_in, ftrs, corr_th):\n",
    "\n",
    "    # selection of the features, which are not 'highly intercorrelated' (correlation is defined by Spearman coefficient);\n",
    "    # pairwise correlation between all the features is calculated,\n",
    "    # from each pair of features, considered as intercorrelated,\n",
    "    # feature with maximum sum of all the pairwise Spearman correlation coefficients is a 'candidate to be dropped'\n",
    "    # for stability of the selected features, bootstrapping approach is used:\n",
    "    # in each bootstrap split, the random subsample, stratified in relation to outcome,\n",
    "    # is formed, based on original observations from input dataset;\n",
    "    # in each bootstrap split, 'candidates to be dropped' are detected;\n",
    "    # for each input feature, its frequency to appear as 'candidate to be dropped' is calculated,\n",
    "    # features, appeared in 50 % of splits as 'candidate to be dropped', are excluded from feature set\n",
    "\n",
    "    # input:\n",
    "    # df_in - input dataframe, containing feature values (dataframe, columns = features, rows = observations),\n",
    "    # ftrs - list of dataframe features, used in analysis (list of feature names - string variables),\n",
    "    # corr_th - threshold for Spearman correlation coefficient, defining each pair of features as intercorrelated (float)\n",
    "\n",
    "    # output:\n",
    "    # non_intercorrelated_features - list of names of features, which did not appear as inter-correlated\n",
    "\n",
    "    corr_matrix = df_in.corr(method='spearman').abs()\n",
    "    mean_absolute_corr = corr_matrix.mean()\n",
    "    intercorrelated_features_set = []\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    high_corrs = upper.where(upper > corr_th).dropna(how='all', axis=1).dropna(how='all', axis=0)\n",
    "\n",
    "    for feature in high_corrs.columns:\n",
    "        mean_absolute_main = mean_absolute_corr[feature]\n",
    "        correlated_with_feature = high_corrs[feature].index[pd.notnull(high_corrs[feature])]\n",
    "        for each_correlated_feature in correlated_with_feature:\n",
    "            mean_absolute = mean_absolute_corr[each_correlated_feature]\n",
    "            if mean_absolute_main > mean_absolute:\n",
    "                if feature not in intercorrelated_features_set:\n",
    "                    intercorrelated_features_set.append(feature)\n",
    "            else:\n",
    "                if each_correlated_feature not in intercorrelated_features_set:\n",
    "                    intercorrelated_features_set.append(each_correlated_feature)\n",
    "\n",
    "    non_intercorrelated_features_set = [e for e in ftrs if e not in intercorrelated_features_set]\n",
    "\n",
    "    print ('Non intercorrelated features: ', non_intercorrelated_features_set)\n",
    "\n",
    "    return non_intercorrelated_features_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4knIjgI6zpgf"
   },
   "source": [
    "We will use a threshold of 0.9 for absolute value of Spearmann's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8663,
     "status": "ok",
     "timestamp": 1687866934858,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "sRitM94Czsek",
    "outputId": "575fe0e9-5a4d-4a3d-80e1-6e7df9296f2a"
   },
   "outputs": [],
   "source": [
    "features_non_intercorrelated = selectNonIntercorrelated(data_train, features_non_zero_var, 0.9)\n",
    "print ('Number of non-intercorrelated features: ', len(...................))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ-vBI2QzuxC"
   },
   "source": [
    "We will perform feature selection using Recursive Feature Elimination (RFE). In this step, feature selection is based on the outcome, so simple models are built and those features that contribute the least to the model are removed recursively. Here you can edit the parameters.\n",
    "\n",
    "<b>You might have heard of variable normalization. Why are we not normalizing the variables (e.g. Z-score normalization)?</b>\n",
    "Hint: e.g. https://stackoverflow.com/questions/57339104/is-normalization-necessary-for-randomforest\n",
    "\n",
    "<b>How many features do we need to select with RFE?\n",
    "Time to discuss pros and cons of using many and a little features.</b>  \n",
    "  \n",
    "There are some rules of thumb on how many features we need in the end:  \n",
    "* $int(\\frac{N_{samples}}{10})$ (Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4, p. 4). New York: AMLBook.)  \n",
    "* $\\sqrt{N_{samples}}$ (Hua, J., Xiong, Z., Lowey, J., Suh, E., & Dougherty, E. R. (2005). Optimal number of features as a function of sample size for various classification rules. Bioinformatics, 21(8), 1509-1515.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1687866937762,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "8nUlfh620JIt",
    "outputId": "2eba1001-78f7-4d41-ebc3-0b17e64197e3"
   },
   "outputs": [],
   "source": [
    "print ('Nmber of samples in training dataset: ', np.sum(outcome_train))\n",
    "print ('Number of features to select according to Abu-Mostafa: ', ................)\n",
    "print ('Number of features to select according to Hua: ', .....................))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggNVDDIS0j3K"
   },
   "source": [
    "Let's go for the lower number of features since our dataset is not large. Below we implement Recursive Feature Elimination, RFE (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html), based on Random Forest Classifier, RFC (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "san1UFHv0k8b"
   },
   "outputs": [],
   "source": [
    "number_selected_features = 6 # Modify\n",
    "estimator = RandomForestClassifier(n_estimators=100, random_state=np.random.seed(0))\n",
    "selector = RFE(estimator, n_features_to_select=number_selected_features, step=1)\n",
    "selector = selector.fit(data_train[features_non_intercorrelated], outcome_train) # Modify outcome_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eQ0lfWX0pdA"
   },
   "source": [
    "And then select the features 'supported' by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1687866995977,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "v_XsyJFs0vKe",
    "outputId": "a4d6f374-a353-497f-a2fb-265a7814142b"
   },
   "outputs": [],
   "source": [
    "support = selector.get_support()\n",
    "selected_features_set = data_train[features_non_intercorrelated].loc[:, support].columns.tolist()\n",
    "\n",
    "print (selected_features_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sZe7WXF0yuI"
   },
   "source": [
    "<b>How many features give the best performance?</b>\n",
    "\n",
    "Time to discuss why we don't just use 100 features!  \n",
    "  \n",
    "The other option could be ranking the features and then building classifier adding +1 feature at every step and checking its performance. We will evaluate the performance with the balanced accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 39686,
     "status": "ok",
     "timestamp": 1687867037551,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "q5Z6lqOJ2Ii8",
    "outputId": "2ace1271-3c70-4c40-a149-819a0ab5cf2f"
   },
   "outputs": [],
   "source": [
    "# ranking the features based on RFE results\n",
    "\n",
    "features_ranks = pd.DataFrame({'Features': features_non_intercorrelated, 'Ranks': selector.ranking_})\n",
    "features_ranks.sort_values(by='Ranks', inplace = True)\n",
    "\n",
    "# taking one best feature first, building the RFC, estimating the performance;\n",
    "# adding +1 next feature, repeating the steps to estimate the performance\n",
    "\n",
    "ftrs_number_tuning = []\n",
    "acc_tuning = []\n",
    "\n",
    "for i in range (1, len(features_ranks)):\n",
    "\n",
    "    ftrs_number_tuning.append(i)\n",
    "    estimator_tuning = RandomForestClassifier(n_estimators=100, random_state=np.random.seed(0))\n",
    "    estimator_tuning.fit(data_train[features_ranks['Features'][:i]], outcome_train) # Modify outcome train\n",
    "    outcome_pred_tuning = estimator_tuning.predict(data_test[features_ranks['Features'][:i]])\n",
    "    acc_tuning.append(balanced_accuracy_score(outcome_test, outcome_pred_tuning))\n",
    "\n",
    "# plotting the results\n",
    "plt.plot(ftrs_number_tuning, acc_tuning)\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('RFC performance')\n",
    "plt.title('RFC performance depending on number of selected features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-7G-Ks52JVd"
   },
   "source": [
    "What can we conclude from the plot? What are the downsides of the presented implementation? Is it correct to train and evaluate the model on the same samples? Is the selected performance metric correct? What other metrics can we use?\n",
    "\n",
    "## Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw1FY2Z7i5Mc"
   },
   "source": [
    "### MODEL 1: RFC\n",
    "\n",
    "We started with RFC while performing RFE, so let us train this model with the selected features and evaluate in on test data.  \n",
    "\n",
    "Training the model (it's possible to vary the parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1687867053618,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "26P08OHd2Tl_",
    "outputId": "1754d120-afbb-42ef-8d2a-654e97e42b56"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=np.random.seed(0))\n",
    "rfc.fit(data_train[selected_features_set], outcome_train) # Modify outcome train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKz5JUeC2YAK"
   },
   "source": [
    "Prediction for the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tvi88TlB2aR6"
   },
   "outputs": [],
   "source": [
    "outcome_pred_rfc = rfc.predict(data_test[selected_features_set]) # Modify what to test on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFGcuGAi2c5_"
   },
   "source": [
    "Performance reporting on some key classification scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1687867059647,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "US3QUiD_2l5j",
    "outputId": "3d7ddaa2-41f0-4dfb-a911-f191238c915b"
   },
   "outputs": [],
   "source": [
    "print (classification_report(........., ............)) # Modify what to report on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxY5UdRS2oA7"
   },
   "source": [
    "Are precision and recall informative metrics? Why don't we report accuracy as a key metric? In which cases accuracy is not suitable scores?  \n",
    "  \n",
    "The other popular metric for classification is Receiver Operating Characteristic (ROC) curve and area under the curve (AUC). We will calculate true positive rates (TPR) and false positive rates (FPR) while varying classification threshold and plot the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 860,
     "status": "ok",
     "timestamp": 1687867074567,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "yZdZn37z2qPQ",
    "outputId": "35123166-a4cb-4160-dbb5-8fbe9746c78a"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(outcome_test, rfc.predict_proba(data_test[selected_features_set])[:, 1])\n",
    "roc_auc = roc_auc_score(outcome_test, rfc.predict_proba(data_test[selected_features_set])[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('RFC ROC curve (AUC = {})'.format(roc_auc))\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QneA8cJZ2sP1"
   },
   "source": [
    "In which cases this metric does not give a correct representation of the model performance? Is AUC always the best metric?\n",
    "Most cetainly not! Especially for unbalanced datasets, AUC can be meaningless.\n",
    "http://www.davidsbatista.net/blog/2018/08/19/NLP_Metrics/\n",
    "\"With imbalanced classes, itâ€™s easy to get a high accuracy without actually making useful predictions. So, accuracy as an evaluation metrics makes sense only if the class labels are uniformly distributed\"    \n",
    "\n",
    "To have a better understanding of model behaviour, we will plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1687867094322,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "HzW4Fgn02xoP",
    "outputId": "0039d83b-600a-4396-b9cf-5292fad24e25"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(..............., .........) # Modify what to report on\n",
    "f = sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix for RFC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YETFriH023pW"
   },
   "source": [
    "\n",
    "\n",
    "### MODEL 2: XGBoost\n",
    "\n",
    "The second model we will build is XGBoost because recently the algorithm was successfull in many machine learning competitions.\n",
    "\n",
    "<b>XGBoost - what is it? It's always best to understand your models. </b>\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_intro.html#\n",
    "\n",
    "Train the model: Here you can chose the max number of boosting iterations, a balance between computing time and accuracy.\n",
    "\n",
    "Presenting the data in the appropriate format for the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc7V2Uxw3EzF"
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(...........[selected_features_set], label=.........) # Modify the labels\n",
    "dtest = xgb.DMatrix(............[selected_features_set], label=.........) # Modify the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0F-YNfT3G_Q"
   },
   "source": [
    "Defining the parameters: the learning objective is logistic regression for binary classification with probability output, the metric is the area under precision-recall curve (why not ROC AUC?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mt119Hy83Lv-"
   },
   "outputs": [],
   "source": [
    "param = {'objective': 'binary:logistic', 'eval_metric': 'aucpr'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsilJkzb3Ohc"
   },
   "source": [
    "We will train the model on the training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5Vdwp5n3Q3k"
   },
   "outputs": [],
   "source": [
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVkWe4n53Sq1"
   },
   "source": [
    "Training the model (number of iterations can be changed here!) and calculating outcomes for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1687867117671,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "JjSYef5k3Vm7",
    "outputId": "e6c989c3-a9c8-4eac-f448-d134e90ac01f"
   },
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "\n",
    "outcome_pred_xgb = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsKuk8pI3YPf"
   },
   "source": [
    "Classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1687867133895,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "Kk4JUhKk3bKc",
    "outputId": "134e7bcb-6052-405e-d4f7-471dfafd884f"
   },
   "outputs": [],
   "source": [
    "print (classification_report(........, ............>0.5)) # Modify what to report on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXlktROy3c7b"
   },
   "source": [
    "ROC and ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1687867158940,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "2Kg5dlpk3gEd",
    "outputId": "765c017f-99b5-471f-996c-bc5f346b34ea"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(......., ..........) # This can be modified as an AUC curve was constructed already above\n",
    "roc_auc = roc_auc_score(........, outcome_pred_xgb)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('XGBoost ROC curve (AUC = {})'.format(roc_auc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeaigfZW3iXE"
   },
   "source": [
    "Create and display the confusion matrix and derived values for the Xgboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1687867176405,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "HtPhR7Jx33CP",
    "outputId": "079e4904-c38a-4452-b6fc-5a5d48a4e734"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(........., outcome_pred_xgb>0.5) # This can be modified as a confusion matrix was constructed already above\n",
    "f = sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJpHgmIW36hA"
   },
   "source": [
    "<b>Almost done! You will now compare the performance of the models.</b>\n",
    "  \n",
    "Which classifier is better?  \n",
    "\n",
    "To compare ROC AUC scores, we will perform a permuation test for the probabilities obtained on the test set for the both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1687867178032,
     "user": {
      "displayName": "Astrid Van Camp",
      "userId": "18403570732781317322"
     },
     "user_tz": -120
    },
    "id": "0fu8MAO44AP8",
    "outputId": "f8805f0b-31cc-4588-9007-3382769abe12"
   },
   "outputs": [],
   "source": [
    "# adapted from https://stackoverflow.com/questions/52373318/how-to-compare-roc-auc-scores-of-different-binary-classifiers\n",
    "#-and-assess-statist\n",
    "\n",
    "def permutation_test_between_clfs(y_test, pred_proba_1, pred_proba_2, nsamples=100):\n",
    "    auc_differences = []\n",
    "    auc1 = roc_auc_score(y_test.ravel(), pred_proba_1.ravel())\n",
    "    auc2 = roc_auc_score(y_test.ravel(), pred_proba_2.ravel())\n",
    "    observed_difference = auc1 - auc2\n",
    "    for _ in range(nsamples):\n",
    "        mask = np.random.randint(2, size=len(pred_proba_1.ravel()))\n",
    "        p1 = np.where(mask, pred_proba_1.ravel(), pred_proba_2.ravel())\n",
    "        p2 = np.where(mask, pred_proba_2.ravel(), pred_proba_1.ravel())\n",
    "        auc1 = roc_auc_score(y_test.ravel(), p1)\n",
    "        auc2 = roc_auc_score(y_test.ravel(), p2)\n",
    "        auc_differences.append(auc1 - auc2)\n",
    "    return observed_difference, np.mean(auc_differences >= observed_difference)\n",
    "\n",
    "print ('Difference, p-value: ',\n",
    "       permutation_test_between_clfs(outcome_test,\n",
    "                                     outcome_pred_xgb,\n",
    "                                     rfc.predict_proba(data_test[selected_features_set])[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmqNYAaL4J8D"
   },
   "source": [
    "After this test, which classifier is better?  \n",
    "  \n",
    "After this you can go back and \"finetune\" the models by changing the parameters you feel comfortable with. See you can increase the model performance. Can you beat the other groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2N0DOwiuWSK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LzuYSO_Nmn43"
   ],
   "provenance": [
    {
     "file_id": "1VvtBkMTKwhYeuyvvAmV676b1fPbYGA8Q",
     "timestamp": 1687869088356
    },
    {
     "file_id": "1sx1E-45ZmLUC50J4FUEazunrGRB19zWs",
     "timestamp": 1687865963534
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
