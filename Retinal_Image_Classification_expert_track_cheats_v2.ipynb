{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Make sure you are running this in google colab.</b>\n",
        "Go to the link: https://drive.google.com/drive/folders/1g3SZoPdl1KQOd8cn9Xdi6k_NlN1QaMfJ?usp=sharing and click on <i> \"add the shortcut to My Drive\" </i>. This will add the shortcut to My Drive.\n",
        "\n",
        "For a detailed step by step procedure, please open this link to follow the instructions: https://docs.google.com/document/d/1H__QmGhzPT2RfralQ--bwLBz3O-rg6L4/edit?usp=sharing&ouid=107456846675416400203&rtpof=true&sd=true\n",
        "\n",
        "<b> Before starting your notebook, click on Runtime: Change Runtime to GPU. If not possible, proceed with the GPU runtime. </b>"
      ],
      "metadata": {
        "id": "kIDDlMauYBki"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUN8aYNWAXzi"
      },
      "source": [
        "# Retinal Fundus Multi-disease classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0CvBtALAXzm"
      },
      "source": [
        "#### <b>Brief description:</b>\n",
        "According to the WHO,  World report on vision 2019, the number of visually impaired people worldwide is estimated to be 2.2 billion, of whom at least 1 billion have a vision impairment that could have been prevented or is yet to be addressed. The world faces considerable challenges in terms of eye care, including inequalities in the coverage and quality of prevention, treatment, and rehabilitation services. Early detection and diagnosis of ocular pathologies would enable forestall of visual impairment. One challenge that limits the adoption of a computer-aided diagnosis tool by the ophthalmologist is, the sight-threatening rare pathologies such as central retinal artery occlusion or anterior ischemic optic neuropathy and others are usually ignored.<br> \n",
        "\n",
        "Retinal Fundus Multi-disease Image Dataset (RFMiD) consists of fundus images captured using three different fundus cameras with 46 conditions annotated through adjudicated consensus of two senior retinal experts.<br>\n",
        "\n",
        "We aim to develop generalizable models for screening retina.\n",
        "\n",
        "| ![picture](https://drive.google.com/uc?export=view&id=1Q2t8UsfyCPv9gkbf4aDupzNnL0QYLbgb)      | ![picture](https://drive.google.com/uc?export=view&id=1gzJ8eGn4rkQQIBUf40FfiPxkfI0a4oEC)                                                                                         |\n",
        "|:-----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n",
        "| Patient with Age-related macular degeneration (ARMD). <br> It's the leading cause of severe, permanent vision loss in people over age 60. <br>It happens when the small central portion of your retina, called the macula, <br> wears down. | Diabetic retinopathy is a complication of diabetes,  caused by high blood  <br> sugar levels damaging the back of the eye (retina). <br> It can cause blindness if <br> left undiagnosed and untreated.|\n",
        "\n",
        "The diseases included in the dataset that are mentioned in the source paper of the dataset \"Retinal Fundus Multi-Disease Image Dataset (RFMiD). We will use this dataset to classify between the following:\n",
        "\n",
        "1. DR - Diabetic retinopathy <br>\n",
        "2. ARMD - Age-related macular degeneration <br>\n",
        "3. Other diseases. <br>\n",
        "4. No disease risk. <br>\n",
        "\n",
        "#### <b>Notebook structure:</b>\n",
        "The script will take you through 7 steps where you'll learn how to train the models. <br>\n",
        " - <b>1. Importing libraries. </b> \n",
        " - <b>2. Getting Your Data Ready. </b> \n",
        " - <b>3. Write your custom dataloader.</b> \n",
        " - <b>4. Define the Network and Training Parameters. </b> \n",
        " - <b>5. Train your network. </b> \n",
        " - <b>6. Test your network. </b> \n",
        " - <b>7. Interpret the trained models. </b> \n",
        "\n",
        "<br> \n",
        "    \n",
        "\n",
        "#### <b>Filling in Missing Values:</b>\n",
        "\n",
        "You will have to fill in some missing values in the notebook. They will help you to better understand the deep learning workflow.\n",
        "\n",
        "#### <b> If you get stuck you can always open the \"cheat\" notebook, but where's the fun in that? </b> :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaDTbZgZHHwD"
      },
      "source": [
        "## Getting Started : Setting up the google drive and installing some packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GQh_cs1a7yG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kde9DH1a7yJ"
      },
      "outputs": [],
      "source": [
        "!pip install captum SimpleITK barbar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6O7myNoAXzr"
      },
      "source": [
        "## 1. Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY_SWJBPAXzr"
      },
      "outputs": [],
      "source": [
        "# general python imports\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "# sklearn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
        "\n",
        "# pytorch imports\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adadelta\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# other python imports\n",
        "import scipy.ndimage\n",
        "from scipy import misc\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from barbar import Bar\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "# albumentations imports for augmentation\n",
        "import albumentations as A\n",
        "\n",
        "# captum for interpretability\n",
        "from captum.attr import LayerGradCam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNuHbEzAXzt"
      },
      "source": [
        "## 2. Getting Your Data Ready.\n",
        "The training images and test images are stored separately. When we read training and test images, we have to make sure that the labels and  images correspond with each other. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqmGeniDxVug"
      },
      "source": [
        "### 2.1 Reading the Train and Test Image Paths and Corresponding Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPA7c5tJAXzu"
      },
      "outputs": [],
      "source": [
        "test_images_path = r\"/content/drive/MyDrive/retinal_disease_classification/Evaluation_Set/Validation\" # path where training .pngs are stored\n",
        "train_images_path = r\"/content/drive/MyDrive/retinal_disease_classification/Training_Set/Training\" # path where test .pngs are stored\n",
        "\n",
        "# reading image paths for training and test sets and sorting them in numerical order \n",
        "# train image paths\n",
        "train_images = [ x for x in os.listdir(train_images_path) if x.endswith(\".png\")]\n",
        "train_images = sorted(train_images, key=lambda f: int(f.split('.')[0]))\n",
        "train_images = np.array([ os.path.join(train_images_path, x) for x in train_images])\n",
        "\n",
        "# test image paths\n",
        "test_images = [ x for x in os.listdir(test_images_path) if x.endswith(\".png\")]\n",
        "test_images = sorted(test_images, key=lambda f: int(f.split('.')[0]))\n",
        "test_images = np.array([os.path.join(test_images_path, x) for x in test_images])\n",
        "\n",
        "# reading the corresponding train and validation labels\n",
        "label_train = pd.read_csv(r\"/content/drive/MyDrive/retinal_disease_classification/Training_Set/RFMiD_Training_Labels.csv\")\n",
        "label_test =  pd.read_csv(r\"/content/drive/MyDrive/retinal_disease_classification/Evaluation_Set/RFMiD_Validation_Labels.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlCnUGPkt-zK"
      },
      "outputs": [],
      "source": [
        "# Checking if the train images and train labels are correctly read\n",
        "print(\"############# Training Image Paths: #############\\n\", train_images[:4])\n",
        "print(\"############# Training Image Labels: #############\\n\", label_train.head())\n",
        "\n",
        "\n",
        "print(\"############# Test Image Paths: #############\\n\", test_images[:4])\n",
        "print(\"############# Test Image Labels: #############\\n\", label_test.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNgXgIQYxYMJ"
      },
      "source": [
        "### 2.2 Assigning the Labels\n",
        "The dataset containing labels corresponding to multiple diseases. Some of the diseases contain only a few images corresponding to the diseases. Here we want to classify between No disease risk, DR, ARMD, and other diseases. We will assign them new labels 0, 1, 2, and 3 respectively. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3rO7IHYRe1O"
      },
      "outputs": [],
      "source": [
        "keys_labels = label_train.keys()     # getting the column name information from the dataframe\n",
        "print(\"The dataset contains labels corresponding to multiple diseases:\")\n",
        "print(keys_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj_ij7zHxh-m"
      },
      "outputs": [],
      "source": [
        "index_disease_risk = 1\n",
        "index_dr = 2\n",
        "index_armd = 3\n",
        "new_labels = [\"no disease risk\", \"diabetic retinopathy\", \"age related macular degeneration\", \"other diseases\"]\n",
        "\n",
        "# This function assigns new labels\n",
        "def assign_labels(arr_labels):\n",
        "\n",
        "  new_labels = list() # storing new labels\n",
        "\n",
        "  for idx in range(arr_labels.shape[0]):\n",
        "     \n",
        "     # assign label for disease risk\n",
        "     if arr_labels[idx][index_disease_risk] == 0:\n",
        "       temp_label = 0\n",
        "\n",
        "     # assign label for disease diabetic retinopathy \n",
        "     elif arr_labels[idx][index_dr] == 1:\n",
        "       temp_label = 1\n",
        "\n",
        "     # assign label for age related macular degeneration\n",
        "     elif arr_labels[idx][index_armd] == 1:\n",
        "       temp_label = 2\n",
        "      \n",
        "     # assign labels if there is a disease risk and it does not belong to ARMD and DR\n",
        "     else:\n",
        "       temp_label = 3\n",
        "\n",
        "     new_labels.append(temp_label)\n",
        "    \n",
        "  return new_labels\n",
        "\n",
        "train_labels = np.vstack(assign_labels(label_train.to_numpy())) # getting the numpy values from the dataframe and assigning new labels\n",
        "test_labels = np.vstack(assign_labels(label_test.to_numpy())) # getting the numpy values from the dataframe and assigning new labels\n",
        "\n",
        "print(\"The new labels are as follows: No Disease - 0, Diabetic Retinopathy - 1, Age Related Macular Degeneration - 2, Other disease - 3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP7NXqyWpLyT"
      },
      "source": [
        "### 2.3 Visualizing Labels and Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpW34NqAct1N"
      },
      "outputs": [],
      "source": [
        "# Visualizing random images from the training dataset\n",
        "for i in range(10):\n",
        "  t_idx = np.random.randint(0, len(train_labels))\n",
        "  print(\" Label : \", train_labels[t_idx], \"-\", new_labels[int(train_labels[t_idx])])\n",
        "  plt.figure()\n",
        "  print(train_images[t_idx])\n",
        "  temp_img = np.flip(cv2.imread(train_images[t_idx]), 2)\n",
        "  print(temp_img.shape)\n",
        "  plt.imshow(temp_img)\n",
        "  plt.show()\n",
        "  print(\"#####################################################################\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIk2HX4UrlN9"
      },
      "source": [
        "### 2.4 Stratified cross Validation\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1SbaPKu9xMech1PZda7ueHrN8C4E08oLY)   \n",
        "\n",
        "Cross-validation is a statistical approach used to evaluate and compare learning algorithms by dividing data into two segments; one is to learn or train a model and the other is used to validate the model. In Stratified cross validation, the folds are made by preserving the percentage of samples for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twBB4Atiskhr"
      },
      "outputs": [],
      "source": [
        "kf = StratifiedKFold(n_splits=5)\n",
        "retinal_kf = [x for x in kf.split(train_images, train_labels)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWrD930atA-V"
      },
      "outputs": [],
      "source": [
        "FOLD_NUM = 0 # defining the fold number for 5-fold cross validation\n",
        "\n",
        "# getting training and validation images\n",
        "cv_train_images= train_images[retinal_kf[FOLD_NUM][0]]\n",
        "cv_val_images = train_images[retinal_kf[FOLD_NUM][1]]\n",
        "\n",
        "# getting training and validation labels \n",
        "cv_train_labels= train_labels[retinal_kf[FOLD_NUM][0]]\n",
        "cv_val_labels = train_labels[retinal_kf[FOLD_NUM][1]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvjbVug5vPu9"
      },
      "outputs": [],
      "source": [
        "print(\"Number of per-class instances in the training set: \\n\")\n",
        "\n",
        "for class_idx, class_val in enumerate(new_labels):\n",
        "  print(\"Idx : \", class_idx, \" - \", class_val)\n",
        "  print(\"number of class instances: \" , np.sum(cv_train_labels == class_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7goX9Ns1lbl"
      },
      "outputs": [],
      "source": [
        "print(\"Number of per-class instances in the Validation set: \\n\")\n",
        "\n",
        "for class_idx, class_val in enumerate(new_labels):\n",
        "  print(\"Idx : \", class_idx, \" - \", class_val)\n",
        "  print(\"number of class instances: \" , np.sum(cv_val_labels == class_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKgIqBUh1rZP"
      },
      "outputs": [],
      "source": [
        "print(\"Number of per-class instances in the test set: \\n\")\n",
        "\n",
        "for class_idx, class_val in enumerate(new_labels):\n",
        "  print(\"Idx : \", class_idx, \" - \", class_val)\n",
        "  print(\"number of class instances: \" , np.sum(test_labels == class_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOwh85715Is"
      },
      "source": [
        "### 2.4 Pre-processing the images\n",
        "We pre-process the images to standardize the image values. Here, we visualize the effect of image resizing and mean standard deviation normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMt27hsK3twH"
      },
      "outputs": [],
      "source": [
        "def pre_process(img_arr):\n",
        "  # min max normalization\n",
        "  img_arr = cv2.resize(img_arr, (450, 300))\n",
        "  img_arr = (img_arr - np.min(img_arr))/(np.max(img_arr) - np.min(img_arr))\n",
        "  return img_arr\n",
        "\n",
        "\n",
        "# Visualizing random images from the training dataset before and after pre-processing\n",
        "for i in range(10):\n",
        "  t_idx = np.random.randint(0, len(train_labels))\n",
        "  print(\" Label : \", train_labels[t_idx], \"-\", new_labels[int(train_labels[t_idx])])\n",
        "\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.title(\"Original Image\")\n",
        "  temp_img = np.flip(cv2.imread(train_images[t_idx]), 2)\n",
        "  plt.imshow(temp_img)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.title(\"Preprocessed image\")\n",
        "  p_temp_img = pre_process(temp_img)\n",
        "  plt.imshow(p_temp_img)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_wh_AYbLnf0"
      },
      "source": [
        "### 2.5 Generating train, validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X137TdVYLpFK"
      },
      "outputs": [],
      "source": [
        "# generating training array\n",
        "train_img_arr = []\n",
        "for img_n in tqdm(cv_train_images):\n",
        "  temp_img_arr = cv2.imread(img_n) # reading the image using opencv\n",
        "  temp_img_arr = np.flip(temp_img_arr, 2)\n",
        "  temp_img_arr = pre_process(temp_img_arr)  # pre_processing the image\n",
        "  train_img_arr.append(np.expand_dims(temp_img_arr, axis=0))\n",
        "train_img_arr = np.vstack(train_img_arr)\n",
        "print(\"The size of the training array is: \", train_img_arr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq0DKKraXSU_"
      },
      "outputs": [],
      "source": [
        "# generating validation array\n",
        "val_img_arr = []\n",
        "for img_n in tqdm(cv_val_images):\n",
        "  temp_img_arr = cv2.imread(img_n)\n",
        "  temp_img_arr = np.flip(temp_img_arr, 2)\n",
        "  temp_img_arr = pre_process(temp_img_arr)  # pre_processing the image\n",
        "  val_img_arr.append(np.expand_dims(temp_img_arr, axis=0))\n",
        "val_img_arr = np.vstack(val_img_arr)\n",
        "print(\"The size of the validation array is: \", val_img_arr.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZduWZQ3rAXzv"
      },
      "source": [
        "## 3. Writing custom dataset, dataloaders and transforms. \n",
        "Pytorch dataset and dataloaders determine how the data is fed into the models. For more information on writing custom dataset and dataloaders, please visit : https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHmvpGqX93Z6"
      },
      "source": [
        "### 3.1 Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GORFNbPPpQGj"
      },
      "outputs": [],
      "source": [
        "# Data Generator Functions\n",
        "class RetinaDataset(Dataset):\n",
        " \n",
        "    def __init__(self, image_arr, label_arr, transform=None):\n",
        "        self.transform = transform\n",
        "        self.image_arr = image_arr\n",
        "        self.label_arr = label_arr\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.image_arr)\n",
        " \n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.image_arr[idx]\n",
        "        label = self.label_arr[idx]\n",
        "\n",
        "        # applying transformation if present\n",
        "        if self.transform is not None:\n",
        "          transformed_image = self.transform(image = image)\n",
        "          image = transformed_image[\"image\"]\n",
        "\n",
        "        image = np.transpose(image, (2,0,1)) # put in the form of channels x width x height\n",
        "        sample = {'image': image, 'label': label}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl8NAuml-Z_h"
      },
      "source": [
        "### 3.2 Augmentation using albumentation. \n",
        "To understand the hyperparameters involved for generating augmentation through transformations, please visit: https://albumentations.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIBti9tP9W9k"
      },
      "outputs": [],
      "source": [
        "training_transform=A.Compose([A.ShiftScaleRotate(shift_limit = 0.1, rotate_limit = 30, always_apply=False, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
        "                              A.HorizontalFlip(p=0.5),\n",
        "                              A.VerticalFlip(p=0.5)])\n",
        "validation_transform=A.Compose([]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tOYHd3AB_Fe"
      },
      "source": [
        "### 3.3 Defining Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_OaWZxPDAwB"
      },
      "outputs": [],
      "source": [
        "param_batch_size = 8\n",
        "\n",
        "# training dataset and dataloader\n",
        "training_dataset=RetinaDataset(train_img_arr, cv_train_labels, transform=training_transform)\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=param_batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "# validation dataset and dataloader\n",
        "validation_dataset = RetinaDataset(val_img_arr, cv_val_labels, transform=validation_transform)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=param_batch_size,shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ox7aZAfJob5"
      },
      "source": [
        "### 3.4 Visualizing the images from the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Zqx0hRuJwb_"
      },
      "outputs": [],
      "source": [
        "for b, sample in enumerate(training_dataloader):\n",
        "    t_img = np.transpose(sample[\"image\"], (0,2,3,1))\n",
        "    t_label = sample[\"label\"]\n",
        "    print(t_img.shape)\n",
        "    \n",
        "    for idx in range(param_batch_size):\n",
        "        print(sample[\"label\"][idx])\n",
        "        plt.figure()\n",
        "        plt.imshow(t_img[idx,:,:,:])\n",
        "        plt.show()\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRhQZq5XAXzy"
      },
      "source": [
        "# 4. Define network and  training parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtnnyMeZ7fBf"
      },
      "source": [
        "### 4.1 Earling Stopping\n",
        "The early stopping class ensures that the best model corresponding to the lowest validation score is stored and if the validation performance does not improve after a certain number of epochs, the model training stops. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTFGlMKI7ilU"
      },
      "outputs": [],
      "source": [
        "# Early Stopping Functions\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE911auKaPV_"
      },
      "source": [
        "### 4.2 Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jrF6gXuaKGu"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "model.classifier = nn.Linear(1280, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axdw8KrVa7yY"
      },
      "source": [
        "### 4.3 Model Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCvGAiula7yZ"
      },
      "outputs": [],
      "source": [
        "patience_es = 5 #patience for early stopping\n",
        "weights = [2, 2, 4, 1] #class weights to account for class imbalance\n",
        "param_lr = 1e-4 #learning rate for the optimizer\n",
        "param_weight_decay = 1e-5 #weight decay for the optimizer\n",
        "num_epochs = 10 # number of epochs we should train the model\n",
        "# choice of learning rate scheduler\n",
        "# choice of optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kC6ed6IbVGD"
      },
      "source": [
        "## 5. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFgroIlabYAd"
      },
      "outputs": [],
      "source": [
        "# mkdir for stored models\n",
        "!mkdir models\n",
        "!mkdir checkpoints\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=patience_es, verbose=True)\n",
        "\n",
        "train_loss_all = []\n",
        "train_acc_all = []\n",
        "val_loss_all = []\n",
        "val_acc_all = []\n",
        "\n",
        "model_name = 'model_workshop'\n",
        "\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=param_lr, weight_decay=param_weight_decay)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 10)\n",
        "\n",
        "# send the model to the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "model = model.to(device)\n",
        "\n",
        "class_weights = torch.FloatTensor(weights).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# training loop\n",
        "training = True\n",
        "epoch = 1\n",
        "try:\n",
        "    while training:\n",
        "    \n",
        "        # epoch specific metrics\n",
        "        train_loss = 0\n",
        "        mask_loss = 0\n",
        "        train_accuracy = 0\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        total_loss = 0\n",
        "        # -----------------------------\n",
        "        # training samples\n",
        "        # -----------------------------\n",
        "        \n",
        "        # set the model into train mode\n",
        "        model.train()\n",
        "        for b, batch in enumerate(Bar(training_dataloader)):\n",
        "\n",
        "\n",
        "                x = batch['image'].to(device).float()\n",
        "                y = batch['label'].to(device).long()\n",
        "\n",
        "                # clear gradients\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # infer the current batch \n",
        "                pred = model(x)\n",
        "                \n",
        "                # compute the loss. \n",
        "                loss = criterion(pred, y.squeeze())\n",
        "                train_loss += loss.item()\n",
        "                \n",
        "                # backward loss and next step\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                # compute the accuracy\n",
        "                pred = pred.max(1, keepdim=True)[1]\n",
        "                batch_accuracy = pred.eq(y.view_as(pred).long())\n",
        "                train_accuracy += (batch_accuracy.sum().item() / np.prod(y.shape))\n",
        "                \n",
        "                \n",
        "        # -----------------------------\n",
        "        # validation samples\n",
        "        # -----------------------------\n",
        "    \n",
        "        # set the model into train mode\n",
        "        model.eval()\n",
        "        for b, batch in enumerate(Bar(validation_dataloader)):\n",
        "\n",
        "                x = batch['image'].to(device).float()\n",
        "                y = batch['label'].to(device).long()\n",
        "\n",
        "                # infer the current batch \n",
        "                with torch.no_grad():\n",
        "                    pred = model(x)\n",
        "                    loss = criterion(pred, y.squeeze())\n",
        "                    val_loss += loss.item()\n",
        "                \n",
        "                    # compute the accuracy \n",
        "                    pred = pred.max(1, keepdim=True)[1]\n",
        "                    batch_accuracy = pred.eq(y.view_as(pred).long())\n",
        "                    val_accuracy += batch_accuracy.sum().item() / np.prod(y.shape) \n",
        "                #scheduler.step(val_loss)\n",
        "\n",
        "        # compute mean metrics\n",
        "        train_loss /= (len(training_dataloader))\n",
        "        train_accuracy /= (len(training_dataloader))\n",
        "        val_loss /= (len(validation_dataloader))\n",
        "        val_accuracy /= (len(validation_dataloader))\n",
        "        early_stopping(val_loss, model)\n",
        "    \n",
        "        train_loss_all.append(train_loss)\n",
        "        train_acc_all.append(train_accuracy)\n",
        "        val_loss_all.append(val_loss)\n",
        "        val_acc_all.append(val_accuracy)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        print('Epoch {:d} train_loss {:.4f} train_acc {:.4f} val_loss {:.4f} val_acc {:.4f}'.format(\n",
        "            epoch, \n",
        "            train_loss, \n",
        "            train_accuracy,\n",
        "            val_loss,\n",
        "            val_accuracy))\n",
        "        # update epochs\n",
        "        epoch += 1\n",
        "        \n",
        "        # save weights\n",
        "        torch.save(model.state_dict(), \n",
        "                  os.path.join('models', 'model' + str(epoch) + '.pth'))\n",
        "\n",
        "        if epoch >= num_epochs:\n",
        "            training = False\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZwP1IxsAXz0"
      },
      "source": [
        "# 6. Testing the Model and Creating Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNNblLMwpm35"
      },
      "source": [
        "### 6.1 Defining the multi-class ROC curve function (One vs. All)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xKWfHf-psRf"
      },
      "outputs": [],
      "source": [
        "def plot_multiclass_roc(preds, y_test, n_classes, figsize=(17, 6)):\n",
        "\n",
        "    # structures\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = metrics.roc_curve(y_test[:, i], preds[:, i])\n",
        "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
        "\n",
        "    # roc for each class\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('Receiver operating characteristic example')\n",
        "\n",
        "    \n",
        "    for i in range(n_classes):\n",
        "        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))\n",
        "\n",
        "    ax.grid(alpha=.4)\n",
        "    ax.legend()\n",
        "    sns.despine()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9zYQMKNpu0J"
      },
      "source": [
        "### 6.2 Testing on the Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp-O9fdzaSel"
      },
      "outputs": [],
      "source": [
        "# loading the best checkpoint\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "if device.type == \"cuda\":\n",
        "  model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "elif device.type == \"cpu\":\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/retinal_disease_classification/checkpoint.pt\", map_location=torch.device('cpu')))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# setting the model in evaluation model\n",
        "model.eval()\n",
        "count = 0\n",
        "\n",
        "# getting predictions\n",
        "pred_t = []\n",
        "label_t = []\n",
        "out_t = []\n",
        "for b, batch in enumerate(Bar(training_dataloader)):\n",
        "    x = batch['image'].to(device).float()\n",
        "    y = batch['label'].detach().numpy()\n",
        "    \n",
        "    # infer the current batch \n",
        "    with torch.no_grad():\n",
        "        pred = model(x)\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        pred_t.append(np.squeeze(F.softmax(pred[i]).cpu().detach().numpy()))\n",
        "        label_t.append(y[i])\n",
        "        out_t.extend(pred.max(1, keepdim=True)[1].cpu().detach().numpy()[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WxGX9bRrVcU"
      },
      "outputs": [],
      "source": [
        "# plotting the multi-class ROC Curve\n",
        "predicted_prob = np.vstack(pred_t)\n",
        "true_labels = np.array(label_t)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "true_labels = true_labels.reshape(len(true_labels), 1)\n",
        "true_labels = onehot_encoder.fit_transform(true_labels)\n",
        "\n",
        "plot_multiclass_roc(predicted_prob, true_labels, n_classes=4, figsize=(10, 10))\n",
        "print(classification_report(label_t, out_t, target_names=[\"no disease risk\", \"diabetic retinopathy\", \"age related macular degeneration\", \"other diseases\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV6pN6X0p17f"
      },
      "source": [
        "### 6.3 Testing on the Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6BKObWSp6fX"
      },
      "outputs": [],
      "source": [
        "# loading the best checkpoint\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "if device.type == \"cuda\":\n",
        "  model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "elif device.type == \"cpu\":\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/retinal_disease_classification/checkpoint.pt\", map_location=torch.device('cpu')))\n",
        "model = model.to(device)\n",
        "\n",
        "# setting the model in evaluation model\n",
        "model.eval()\n",
        "count = 0\n",
        "\n",
        "# getting predictions\n",
        "pred_t = []\n",
        "label_t = []\n",
        "out_t = []\n",
        "for b, batch in enumerate(Bar(validation_dataloader)):\n",
        "    x = batch['image'].to(device).float()\n",
        "    y = batch['label'].detach().numpy()\n",
        "    \n",
        "    # infer the current batch \n",
        "    with torch.no_grad():\n",
        "        pred = model(x)\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        pred_t.append(np.squeeze(F.softmax(pred[i]).cpu().detach().numpy()))\n",
        "        label_t.append(y[i])\n",
        "        out_t.extend(pred.max(1, keepdim=True)[1].cpu().detach().numpy()[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYsVj4vDsZoK"
      },
      "outputs": [],
      "source": [
        "# plotting the multi-class ROC Curve\n",
        "predicted_prob = np.vstack(pred_t)\n",
        "true_labels = np.array(label_t)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "true_labels = true_labels.reshape(len(true_labels), 1)\n",
        "true_labels = onehot_encoder.fit_transform(true_labels)\n",
        "plot_multiclass_roc(predicted_prob, true_labels, n_classes=4, figsize=(10, 10))\n",
        "print(classification_report(label_t, out_t, target_names=[\"no disease risk\", \"diabetic retinopathy\", \"age related macular degeneration\", \"other diseases\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zhDZMrp7OY"
      },
      "source": [
        "### 6.4 Testing on the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH9X1B2Fp9zd"
      },
      "outputs": [],
      "source": [
        "# loading the best checkpoint\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "if device.type == \"cuda\":\n",
        "  model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "elif device.type == \"cpu\":\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/retinal_disease_classification/checkpoint.pt\", map_location=torch.device('cpu')))\n",
        "model = model.to(device)\n",
        "\n",
        "# setting the model in evaluation model\n",
        "model.eval()\n",
        "idx = 0\n",
        "\n",
        "# getting predictions\n",
        "pred_t = []\n",
        "label_t = []\n",
        "out_t = []\n",
        "\n",
        "for  img_n in tqdm(test_images):\n",
        "\n",
        "    # pre-processing the test image\n",
        "    temp_img_arr = cv2.imread(img_n)\n",
        "    temp_img_arr = np.flip(temp_img_arr, 2)\n",
        "    temp_img_arr = np.transpose(np.expand_dims(pre_process(temp_img_arr), axis=0), (0,3,1,2))\n",
        "    temp_img_arr = torch.tensor(temp_img_arr)\n",
        "\n",
        "    x = temp_img_arr.to(device).float()\n",
        "    y = test_labels[idx]\n",
        "    \n",
        "    # infer the current batch \n",
        "    with torch.no_grad():\n",
        "        pred = model(x)\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        pred_t.append(np.squeeze(F.softmax(pred[i]).cpu().detach().numpy()))\n",
        "        label_t.append(y[i])\n",
        "        out_t.extend(pred.max(1, keepdim=True)[1].cpu().detach().numpy()[i])\n",
        "        \n",
        "    idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eh8gosRvbAe"
      },
      "outputs": [],
      "source": [
        "# plotting the multi-class ROC Curve\n",
        "predicted_prob = np.vstack(pred_t)\n",
        "true_labels = np.array(label_t)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "true_labels = true_labels.reshape(len(true_labels), 1)\n",
        "true_labels = onehot_encoder.fit_transform(true_labels)\n",
        "\n",
        "plot_multiclass_roc(predicted_prob, true_labels, n_classes=4, figsize=(10, 10))\n",
        "print(classification_report(label_t, out_t, target_names=[\"no disease risk\", \"diabetic retinopathy\", \"age related macular degeneration\", \"other diseases\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBNBH5saa7yd"
      },
      "source": [
        "### 6.5 Testing with Test time augmentation (TTA)\n",
        "Test time augmentation (TTA) is a popular technique in computer vision. TTA aims at boosting the model accuracy by using data augmentation on the inference stage. The idea behind TTA is simple: for each test image, we create multiple versions that are a little different from the original image and average the output predictions for each of those transformed images, resutling in a final prediction more robust. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAAtCU_va7yd"
      },
      "outputs": [],
      "source": [
        "# test transform to apply TTA\n",
        "test_transform =A.Compose([A.ShiftScaleRotate(shift_limit = 0.1, rotate_limit = 30, always_apply=False, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
        "                              A.HorizontalFlip(p=0.5),\n",
        "                              A.VerticalFlip(p=0.5)])\n",
        "tta_num = 5\n",
        "\n",
        "# loading the best checkpoint\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "if device.type == \"cuda\":\n",
        "  model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "elif device.type == \"cpu\":\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/retinal_disease_classification/checkpoint.pt\", map_location=torch.device('cpu')))\n",
        "model = model.to(device)\n",
        "\n",
        "# setting the model in evaluation model\n",
        "model.eval()\n",
        "idx = 0\n",
        "\n",
        "# getting predictions\n",
        "pred_t = []\n",
        "label_t = []\n",
        "out_t = []\n",
        "\n",
        "for  img_n in tqdm(test_images):\n",
        "\n",
        "    # pre-processing the test image\n",
        "    temp_img_arr = cv2.imread(img_n)\n",
        "    temp_img_arr = np.flip(temp_img_arr, 2)\n",
        "    temp_img_arr = pre_process(temp_img_arr)\n",
        "\n",
        "    pred_s = []\n",
        "    for i in range(5):\n",
        "        temp_img_arr_t = test_transform(image=temp_img_arr)[\"image\"]\n",
        "        temp_img_arr_t = np.transpose(np.expand_dims(temp_img_arr_t, axis=0), (0,3,1,2))\n",
        "        temp_img_arr_t = torch.tensor(temp_img_arr_t)\n",
        "\n",
        "        x = temp_img_arr_t.to(device).float()\n",
        "        y = test_labels[idx]\n",
        "\n",
        "        # infer the current batch \n",
        "        with torch.no_grad():\n",
        "            pred = model(x)\n",
        "            pred_s.append(np.squeeze(F.softmax(pred).cpu().detach().numpy()))\n",
        "        \n",
        "    pred_s_t = np.sum(pred_s, axis = 0)/tta_num\n",
        "    \n",
        "    pred_t.append(pred_s_t)\n",
        "    label_t.append(y)\n",
        "    out_t.extend([np.argmax(pred_s_t, axis=0)])\n",
        "        \n",
        "    idx += 1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6ExIEZYa7yd"
      },
      "outputs": [],
      "source": [
        "# plotting the multi-class ROC Curve\n",
        "predicted_prob = np.vstack(pred_t)\n",
        "true_labels = np.array(label_t)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "true_labels = true_labels.reshape(len(true_labels), 1)\n",
        "true_labels = onehot_encoder.fit_transform(true_labels)\n",
        "\n",
        "plot_multiclass_roc(predicted_prob, true_labels, n_classes=4, figsize=(10, 10))\n",
        "print(classification_report(label_t, out_t, target_names=[\"no disease risk\", \"diabetic retinopathy\", \"age related macular degeneration\", \"other diseases\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1plUefaa7ye"
      },
      "source": [
        "## 7. Grad-CAMS for interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U01KPOo1a7ye"
      },
      "source": [
        "Grad-CAMs are used for interpretability. They highlight the area that the model considers important for prediction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwhIVsuCa7ye"
      },
      "outputs": [],
      "source": [
        "# loading the best checkpoint\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "if device.type == \"cuda\":\n",
        "  model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "elif device.type == \"cpu\":\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/retinal_disease_classification/checkpoint.pt\", map_location=torch.device('cpu')))\n",
        "model = model.to(device)\n",
        "\n",
        "# setting the model in evaluation model\n",
        "model.eval()\n",
        "count = 0\n",
        "\n",
        "# defining the GradCAM layer\n",
        "gradcam = LayerGradCam(model, model.features[15])\n",
        "\n",
        "# getting predictions\n",
        "for b, batch in enumerate(Bar(validation_dataloader)):\n",
        "    x = batch['image'].to(device).float()\n",
        "    y = batch['label'].detach().numpy()\n",
        "    \n",
        "    # infer the current batch \n",
        "    with torch.no_grad():\n",
        "        pred = model(x)\n",
        "\n",
        "    for i in range(len(y)):\n",
        "        predicted_label = np.argmax(np.squeeze(F.softmax(pred[i]).cpu().detach().numpy()), axis=0)\n",
        "        true_label = np.squeeze(y[i])\n",
        "        \n",
        "        if true_label == predicted_label:\n",
        "            print(\"\\n Label: \", true_label, \"-\", new_labels[true_label])\n",
        "            \n",
        "            # check the gradcam for the correct prediction\n",
        "            attributions = gradcam.attribute(torch.unsqueeze(x[i], 0), target=torch.tensor(true_label))\n",
        "            attributions = attributions.cpu().detach().numpy()\n",
        "            attributions = np.transpose(attributions[0], (1,2,0)) \n",
        "            attributions = (attributions - np.min(attributions))/(np.max(attributions) - np.min(attributions))\n",
        "            attributions = cv2.resize(attributions,(450,300))\n",
        "            \n",
        "            plt.figure(figsize=(10,10))\n",
        "            plt.subplot(1,3,1)\n",
        "            plt.imshow(np.transpose(batch['image'][i], (1,2,0)))\n",
        "            plt.title(\"Original Image\")\n",
        " \n",
        "            plt.subplot(1,3,2)\n",
        "            plt.imshow(attributions, cmap=plt.cm.RdBu_r)\n",
        "            plt.title(\"Grad-CAM\")\n",
        "            \n",
        "            plt.subplot(1,3,3)\n",
        "            img = np.transpose(batch['image'][i], (1,2,0)).numpy()\n",
        "            norm_img = (img - np.min(img))/(np.max(img) - np.min(img))\n",
        "            plt.imshow(norm_img)\n",
        "            plt.imshow(attributions, alpha = 0.5, cmap=plt.cm.RdBu_r)\n",
        "            plt.title(\"Overlayed Grad-CAM\")\n",
        "            plt.show()\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Retinal_Image_Classification_expert_track_cheats_v2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python (my_env)",
      "language": "python",
      "name": "my_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}