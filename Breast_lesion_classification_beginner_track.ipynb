{"cells":[{"cell_type":"markdown","metadata":{"id":"6_Qnyn_-tyXF"},"source":["# <b> Make sure you are running this in google colab.</b>\n","Go to the link: https://drive.google.com/drive/folders/11eX89jZEuR0yWtBWwzLuIZDqQmsyXEcT?usp=sharing and click on <i> \"add the shortcut to My Drive\" </i>. This will add the shortcut to My Drive.\n","\n","For a detailed step by step procedure, please open this link to follow the instructions: https://docs.google.com/document/d/1uhH_5W_Aoa8zlyMSvmU_yuhyJJgR6-7l/edit?usp=sharing&ouid=107456846675416400203&rtpof=true&sd=true\n","\n","<b> Before starting your notebook, click on Runtime: Change Runtime to GPU (connect to a hosted runtime i.e., GPU) . If not possible, proceed with the CPU runtime. </b>"]},{"cell_type":"markdown","metadata":{"id":"CtN4zFK7xfOn"},"source":["# Deep Learning for beginners\n","## Classification of suspicious breast lesions"]},{"cell_type":"markdown","metadata":{"id":"z-QPKKNdxfOz"},"source":["### This is a private dataset of Contrast Enhanced Mammography (CEM) kindly supplied by MUMC+ for the purposes of this course only. Please make sure you delete image data after the workshop."]},{"cell_type":"markdown","metadata":{"id":"vb9BDaoHxpsb"},"source":["## Getting Started : Setting up the google drive and installing some packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RdFrQY72AV1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687871128725,"user_tz":-120,"elapsed":15436,"user":{"displayName":"Shruti Mali","userId":"03722643637228962593"}},"outputId":"b3f211d4-0c79-45aa-f8bf-e593697c6a9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gm7KYOO53iNz"},"outputs":[],"source":["!pip install -q livelossplot"]},{"cell_type":"markdown","metadata":{"id":"r50oHO1QxfO1"},"source":["## Description of the dataset\n","* The dataset is comprised of 997 MUMC patients that were recalled after a screening mammography.\n","* Each patient has two views of CEM images  - Caudal-cranial (CC) and mediolateral oblique view (MLO)\n","* Each view has two types of image - the low energy image (almost like a standard mammo) and the recombined image\n","* Masses have pathological confirmation of status, labeled 0 for benign and 1 for malignant\n","* The data has been resized to 256*256 crop around the tumor while maitaining the same greyscale values\n","* the arrays have two channels containing the (1) low energy image and (2) the recombined low and high energy images\n"]},{"cell_type":"markdown","metadata":{"id":"Y5u4tvA_xfO2"},"source":["![picture](https://drive.google.com/uc?export=view&id=1NcUiIikQX4jxCQXgCLkvQHRKlYj8g6ll)  "]},{"cell_type":"markdown","metadata":{"id":"Aa6PG24PxfO4"},"source":["#### <b>Notebook structure:</b>\n","The script will take you through 5 steps where you'll learn how to train the models. <br>\n"," - <b>1. Importing libraries. </b>\n"," - <b>2. Getting Your Data Ready. </b>\n"," - <b>3. Define the Network. </b>\n"," - <b>4. Train your network. </b>\n"," - <b>5. Test your network. </b>\n","\n","<br>\n","\n","\n","#### <b>Filling in Missing Values:</b>\n","\n","You will have to fill in some missing values in the notebook. They will help you to better understand the deep learning workflow.\n","\n","#### <b> If you get stuck you can always open the \"cheat\" notebook, but where's the fun in that? </b> :-)"]},{"cell_type":"markdown","metadata":{"id":"ZJbDQFoXxfO7"},"source":["## 1. Import librairies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oO513u-GxfPA"},"outputs":[],"source":["#desactivate warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#import utilities\n","import numpy as np\n","import pandas as pd\n","from glob import glob\n","import os\n","import shutil\n","from tqdm import tqdm_notebook as tqdm\n","\n","#import images\n","from skimage.io import imread\n","import matplotlib.pyplot as plt\n","\n","#import\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","#import keras functions\n","import tensorflow as tf\n","import keras.backend as K\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.nasnet import NASNetMobile\n","from tensorflow.keras.applications.xception import Xception\n","from tensorflow.keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Input, Concatenate, GlobalMaxPooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n","from tensorflow.keras.optimizers import Adam\n","from livelossplot.tf_keras import PlotLossesCallback"]},{"cell_type":"markdown","metadata":{"id":"sh9KWp4UxfPC"},"source":["## 2. Getting your data ready"]},{"cell_type":"markdown","metadata":{"id":"Tc5GSJpqvXK2"},"source":["### 2.1 Visualizing the data contents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sw0Za7p3xfPE","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687871169637,"user_tz":-120,"elapsed":701,"user":{"displayName":"Shruti Mali","userId":"03722643637228962593"}},"outputId":"2eb88691-37ef-4b45-92cf-1048c9de6a07"},"outputs":[{"output_type":"stream","name":"stdout","text":["                              image_ID position orientation  classification\n","0     ID_LeckFg19Qxio7VJiy7Wu7vpvD.npy        R         MLO               1\n","1     ID_H5ViU9RHKQO74w8Cq8gtZBLkW.npy        R          CC               1\n","2     ID_srgTQm5KBFIH45HtaahcXm66V.npy        R         MLO               0\n","3     ID_2yxLITGuMLRO2UkSrLfKE5l1t.npy        R          CC               0\n","4     ID_6EBGfrbMC5HhWyCNEGdZrmtbA.npy        L         MLO               0\n","...                                ...      ...         ...             ...\n","1989  ID_uB9NDT2Tck4ec48l1iV3GwKfC.npy        L          CC               0\n","1990  ID_xaQ4byQ3ovcmc1X3oHtGAoi4K.npy        R         MLO               1\n","1991  ID_9EWw4bRgn32ovvxDo3JqTy0cu.npy        R          CC               1\n","1992  ID_9ZlE026gbum6IgmhypRs9es5r.npy        R         MLO               1\n","1993  ID_o2NNSrjzBdhnLlCnIxHD4xdab.npy        R          CC               1\n","\n","[1994 rows x 4 columns]\n"]}],"source":["folder_pth = r\"/content/drive/MyDrive/breast_lesion_classification\"\n","\n","csv_path = os.path.join(folder_pth, r\"df_for_workshop.csv\")\n","info_csv = pd.read_csv(csv_path)\n","print(info_csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EY2frPtOxfPF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687871169638,"user_tz":-120,"elapsed":10,"user":{"displayName":"Shruti Mali","userId":"03722643637228962593"}},"outputId":"25819e46-25ec-46a8-ee09-8d294658be25"},"outputs":[{"output_type":"stream","name":"stdout","text":["The targets in the classification variable:\n"]},{"output_type":"execute_result","data":{"text/plain":["0    1\n","1    1\n","2    0\n","3    0\n","4    0\n","Name: classification, dtype: int64"]},"metadata":{},"execution_count":7}],"source":["#Assign the target variable\n","classification = info_csv.classification\n","print(\"The targets in the classification variable:\")\n","classification.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSS5iZyJxfPF","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["bd6740eeb4454a14b917b4e7930ff106","b7e965d0b76c426ca2c3acc83c902d8d","ce099dd22061425cb11605de93fa2ec8","4b891624330647bb8edefdc8fa8d5f31","3e49bcab159f4df0bcec1eb8df399b7b","c2cd225182ff45b49bc7280c23a810d6","31dceaa5d3cb4d4dbbde686f966535ef","2bc8c7037021485284325e09f71f69de","450518c663ea44a9a5d192857b9ec16d","cf30b0e711eb4036aeeda764f1fdbc09","a8c3eb1aa2454fce923b631b3fde52ae"]},"outputId":"ad9a9b04-025e-417a-fdf5-f4052b39005f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1994 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6740eeb4454a14b917b4e7930ff106"}},"metadata":{}}],"source":["#load images\n","image_ID = info_csv[\"image_ID\"] # getting image IDs from the csv file\n","path_npy = os.path.join(folder_pth, r\"dataset_for_workshop\")\n","\n","\n","# reading the training patches in the Variable X using np.load\n","X = []\n","for image_id in tqdm(list(image_ID)):\n","    X.append(np.load(os.path.join(path_npy,image_id)))\n","\n","X = np.array(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cxAH380xfPH"},"outputs":[],"source":["print(\"the size of the dataset is:\", X.shape)\n","\n","#extract shape parameter\n","IMAGE_SIZE = X[0].shape\n","N_IMAGES = len(X)\n","print(\"Number of images:\", N_IMAGES)\n","print(\"Image size:\",IMAGE_SIZE[:2])\n","print(\"Number of channels:\", IMAGE_SIZE[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsB2ffySxfPI"},"outputs":[],"source":["# Using the 'train_test_split' function from the sklearn Python package,\n","# we divided the dataset into a training set and a testing set,\n","# opting for a 70/30 split.\n","X_train, X_test, y_train, y_test = train_test_split(X,classification,test_size=0.3, random_state=42)\n","print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"dOpMQeFZxfPJ"},"source":["## 2.2. Making Training and Testing Data Arrays based on the train_test_split\n","We have split train and test data into two folders. Make two train and test arrays containing the image patches of size 256 x 256."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMZkywPKxfPK"},"outputs":[],"source":["train_ID = os.listdir(os.path.join(folder_pth, \"train\"))\n","X_train = []\n","y_train = []\n","for image_id in tqdm(list(train_ID)):\n","    X_train.append(np.load(os.path.join(folder_pth, \"train\",image_id)))\n","    y_train.append(classification[image_ID == image_id].values[0])\n","X_train = np.array(X_train)\n","\n","test_ID = os.listdir(os.path.join(folder_pth, \"test\"))\n","X_test = []\n","y_test = []\n","for image_id in tqdm(list(test_ID)):\n","    X_test.append(np.load(os.path.join(folder_pth, \"test\",image_id)))\n","    y_test.append(classification[image_ID == image_id].values[0])\n","\n","X_test = np.array(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCib_WZyxfPL"},"outputs":[],"source":["print(\"X_train shape:\", X_train.shape)\n","print(\"X_test shape:\", X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrzTtix9xfPL"},"outputs":[],"source":["#extract shape parameter\n","IMAGE_SHAPE = X_train[0].shape\n","IMAGE_SIZE = X_train[0].shape[1]\n","print(\"Image shape:\",IMAGE_SHAPE[:2])\n","print(\"Number of channels:\", IMAGE_SHAPE[-1])"]},{"cell_type":"markdown","metadata":{"id":"pFOEXoT4xfPL"},"source":["## 2.3: Preprocessing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDzuEe-hxfPL"},"outputs":[],"source":["#normalisation coef, first mean second std\n","norm_coeff_re=[897,728] #norm_coeff_re : for the recombined image, first channel of the array\n","norm_coeff_le=[770,847] #norm_coeff_le : for the low energy image, second channel of the array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJ5nZQCDxfPM"},"outputs":[],"source":["#normalize images (use coefficients)\n","new_X_train = []\n","for i, x in enumerate(X_train):\n","    x_le = (x[:,:,1]-norm_coeff_le[0])/norm_coeff_le[1] # z-score\n","    x_re = (x[:,:,0]-norm_coeff_re[0])/norm_coeff_re[1] # z-score\n","    new_X_train.append(np.concatenate((np.expand_dims(x_re, axis=2), np.expand_dims(x_le, axis=2)),axis = 2)) #concatenate channels on the 3rd axis\n","\n","new_X_test = []\n","for i, x in enumerate(X_test):\n","    x_le = (x[:,:,1]-norm_coeff_le[0])/norm_coeff_le[1] # z-score\n","    x_re = (x[:,:,0]-norm_coeff_re[0])/norm_coeff_re[1] # z-score\n","    new_X_test.append(np.concatenate((np.expand_dims(x_re, axis=2), np.expand_dims(x_le, axis=2)),axis = 2)) #concatenate channels on the 3rd axis\n","\n","new_X_train = np.array(new_X_train)\n","new_X_test = np.array(new_X_test)\n","\n","print(\"new_X_train shape: \", ... ) # FILL IN THE VALUE\n","print(\"new_X_test shape: \", ... ) ## FILL IN THE VALUE"]},{"cell_type":"markdown","metadata":{"id":"RhtbMxRIxfPM"},"source":["What you should see: <br>\n","new_X_train shape:  (1395, 256, 256, 2) <br>\n","new_X_test shape:  (599, 256, 256, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHpl3E1WxfPM"},"outputs":[],"source":["#using only one channel for the Xception model\n","X_train = np.expand_dims(new_X_train[:,:,:,0],axis=3)\n","print(X_train.shape)\n","\n","X_test = ... ## FILL IN THE VALUE Do it in the similar way as you did for the X_train - use only one channel.\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"uN9GRmjzzW3a"},"source":["## 2.4 Visualizing the Slices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VUXKbmgxfPN"},"outputs":[],"source":["# sanity check: check how the data looks like,\n","# change the range parameters and X_test for X_train\n","\n","for i in range(10):\n","    idx = np.random.randint(0, len(X_test))\n","    plt.figure()\n","    plt.imshow(np.squeeze(X_test[idx,:,:]/255))\n","    plt.title(\"Label - \" + str(y_test[idx]))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_b4y3sczxfPN"},"source":["## 3. Define the Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtE-CAWRxfPO"},"outputs":[],"source":["#enter the number of units for the dense layer\n","\n","def CNN_xception(input_shape):\n","    base_model = Xception(weights=None, include_top=False,input_shape=input_shape,pooling = 'avg')\n","    #hint : change the last layers if you built your own model\n","    x = base_model.output\n","    print(x.shape)\n","    x = Dense(1024, activation='relu')(x) #choose number of units for dense layer, 1024 for pre-loaded weights\n","    print(x.shape)\n","    predictions = Dense(1, activation='sigmoid')(x)\n","    print(predictions.shape)\n","\n","    model = Model(inputs=base_model.input, outputs=predictions)\n","    return model\n","\n","callbacks_list = [PlotLossesCallback()]\n","\n","model1 = CNN_xception((IMAGE_SIZE, IMAGE_SIZE,1))\n","\n","#model architecture\n","model1.summary()"]},{"cell_type":"markdown","metadata":{"id":"v4sV-4SBxfPP"},"source":["### 4. Train the Model"]},{"cell_type":"markdown","metadata":{"id":"AqJ0-6Oo0ZlK"},"source":["### 4.1 Mode 1: Training the model using GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_AiwgfBxfPP"},"outputs":[],"source":["#optimizer can be found https://keras.io/optimizers/\n","model1.compile(optimizer= Adam(lr= ... ),loss='binary_crossentropy',metrics=['accuracy']) ## FILL IN THE VALUE What would be the appropriate learning rate?\n","\n","#choose batch size and epochs number\n","history =model1.fit(X_train/255,  np.array(y_train), batch_size= ... , epochs=10 ,\n","                    validation_data=( X_test/255, np.array(y_test)),callbacks=callbacks_list) ## What would be the appropriate learning rate? ## FILL IN THE VALUE"]},{"cell_type":"markdown","metadata":{"id":"k-jrzLk5xfPQ"},"source":["### 4.2: Mode 2: Use the pre-trained weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFKuY89WxfPQ"},"outputs":[],"source":["# use this sell if you can not train the model using the GPUs\n","model1.load_weights(os.path.join(folder_pth, r\"checkpoint.hdf5\"))"]},{"cell_type":"markdown","metadata":{"id":"hRthDL-1xfPQ"},"source":["## 5: Test the model\n","### 5.1 Get the Predictions on the Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCDwRtZSxfPQ"},"outputs":[],"source":["# ROC validation plot\n","\n","predictions = []\n","for x in tqdm(X_test):\n","    predictions.append(model1.predict(np.expand_dims(x,axis=0)/255))\n","predictions = np.array(predictions).flatten()"]},{"cell_type":"markdown","metadata":{"id":"presRqNq3avH"},"source":["### 5.2 plot the ROC curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTqp8R0ixfPR"},"outputs":[],"source":["false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, predictions) #use roc_curve function from sklearn library\n","area_under_curve = auc(false_positive_rate, true_positive_rate) #use auc function from sklearn library\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(false_positive_rate, true_positive_rate, label='AUC = {:.3f}'.format(area_under_curve))\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.title('ROC curve')\n","plt.legend(loc='best')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YvsHLHuh6QA3"},"source":["### 5.3 Classification Report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1hClM20I6Onj"},"outputs":[],"source":["classification_threshold = 0.5\n","true_labels = y_test\n","predicted_labels = predictions > classification_threshold\n","\n","print(classification_report(true_labels, predicted_labels))"]},{"cell_type":"markdown","metadata":{"id":"Fl74mOx0xfPR"},"source":["### 5.4 Test the model with test time augmentation (TTA) to improve results\n","\n","Documentation: https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0sLxYgfxfPR"},"outputs":[],"source":["#can change the transformations see ImageDataGenerator documentations\n","train_datagen = ImageDataGenerator(\n","        rescale=1./255,\n","        shear_range=...,\n","        vertical_flip=True,\n","        horizontal_flip=True) ## FILL IN THE VALUE Define the shear range for the test time augmentation (it goes from 0 to 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhUumJCgxfPS"},"outputs":[],"source":["#choose number of times you apply transformation to the test set\n","tta_idx = 4\n","tta_prediction = np.empty((tta_idx,) + predictions.shape[:] + (1,)) #initialize with the former predictions\n","for i in tqdm(range(tta_idx)):\n","    val_it_plain = train_datagen.flow(X_test,y_test, batch_size=len(y_test), shuffle = False)\n","    x,y = val_it_plain.next()\n","    tta_prediction[i] = model1.predict(x)\n","\n","tta_prediction=np.array(tta_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlRCFqnSxfPT"},"outputs":[],"source":["mean_prediction = np.mean(tta_prediction,axis = 0).reshape(len(y_test))\n","\n","# ROC validation plot with TTA\n","false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, mean_prediction) #use roc_curve function from sklearn library\n","area_under_curve = auc(false_positive_rate, true_positive_rate) #use auc function from sklearn library\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(false_positive_rate, true_positive_rate, label='AUC = {:.3f}'.format(area_under_curve))\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.title('ROC curve')\n","plt.legend(loc='best')\n","#plt.savefig(ROC_PLOT_FILE, bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbFqFpwy9Bcg"},"outputs":[],"source":["classification_threshold = ... ## FILL IN THE VALUE Define the classification threshold, it ranges from 0 to 1\n","true_labels = y_test\n","predicted_labels = mean_prediction > classification_threshold\n","\n","print(classification_report(true_labels, predicted_labels))"]},{"cell_type":"markdown","metadata":{"id":"L60PLDWFxfPT"},"source":["If you arrived here, well done ! Now you can play with the data and try new models"]},{"cell_type":"markdown","metadata":{"id":"TleKRXcpxfPT"},"source":["# Second part: try to implement your own model with 2 channels\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AogCRMGtxfPT"},"source":["Suggestion of changes: <br>\n","* you can load separetly the views (CC and MLO) and train on those subsets\n","* during pre-processing of the data you can implement an other normalization method, filtering, rebinning, data augmentation\n","* for training the model you can try a different base model: vgg16, densenet101, inception ... and pre-load the weights from ImageNet\n","* after loading a base model, you can choose to add some more layer, change the number of units\n","* try different parameters, you can change the learning rate, the loss function, the batch size, the optimizer..."]},{"cell_type":"markdown","metadata":{"id":"bmCi4u5MxfPU"},"source":["## In case the images cannot be load entirely in the memory\n","The generator function load the data batch by batch. To train the model, it needs to be used with model1.fit_generator()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcB44cSlxfPU"},"outputs":[],"source":["# def generator(gen_type, batch_size = 10):\n","\n","#     assert gen_type in [\"train\",\"test\"], \"Allowed gen_type: train or test\"\n","\n","#     ids = os.listdir(gen_type)\n","\n","#     while True:\n","\n","#         X = []\n","#         y = []\n","\n","#         offset = np.random.randint(0,len(ids)-batch_size)\n","#         batch_ids = ids[offset:offset+batch_size]\n","\n","#         for image_id in batch_ids:\n","\n","#             X.append(np.load(os.path.join(gen_type,image_id))) #possible that there might be cache files and stuff, so may be check the extension as well for train_Id\n","#             y.append(classification[image_ID == image_id].values[0])\n","\n","\n","#         yield np.array(X),np.array(y)\n","\n","# train_gen = generator(\"train\")\n","# test_gen = generator(\"test\")\n","\n","# model1.fit_generator(train_gen, steps_per_epoch=2000,\n","#     epochs=20,\n","#     validation_data=test_gen,\n","#     validation_steps=800)"]},{"cell_type":"markdown","source":[" 🎨 **Auxillary task: Play around with different Keras in-built models**\n","\n","Reference: https://www.tensorflow.org/api_docs/python/tf/keras/applications"],"metadata":{"id":"cAwEyGkXT-6v"}},{"cell_type":"code","source":["#enter the number of units for the dense layer\n","tf.keras.backend.clear_session()\n","\n","def CNN_trial(input_shape):\n","  #play with different keras base models\n","    base_model = tf.keras.applications................(weights=None,             # try using any keras model here, refer the previous code block that defines the model\n","                                                       include_top=False,\n","                                                       input_shape=input_shape,\n","                                                       pooling = 'avg')\n","    #hint : change the last layers if you built your own model\n","    x = base_model.output\n","    print(x.shape)\n","    x = Dense(1024, activation='relu')(x) #choose number of units for dense layer, 1024 for pre-loaded weights\n","    print(x.shape)\n","    predictions = Dense(1, activation='sigmoid')(x)\n","    print(predictions.shape)\n","\n","    model = Model(inputs=base_model.input, outputs=predictions)\n","    return model\n","\n","\n","model2 = CNN_trial((IMAGE_SIZE, IMAGE_SIZE,1))\n","\n","#model architecture\n","print(model2.summary())\n","\n","# uncomment for pre-training\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n","model2.compile(optimizer= Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                                  patience=50,\n","                                                  verbose=0,\n","                                                  mode='min',\n","                                                  )\n","model_checkpoint_path = os.path.join(r\"./trial_checkpoint.hdf5\")\n","# load weights of the model\n","# model1.load_weights(model_checkpoint_path)\n","\n","check_point = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path,\n","                                                 monitor='val_loss',\n","                                                 verbose=1,\n","                                                 save_best_only=True,\n","                                                 mode='min',\n","                                                 )\n","reduce_lrplateau = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,\n","                                     min_delta=0.0001, min_lr=1e-6, verbose=1)\n","pretrain_callbacks = [check_point, early_stopping, reduce_lrplateau, PlotLossesCallback()]\n","\n","history =model2.fit(X_train,  np.array(y_train), batch_size=4, epochs=1000,\n","                    validation_data=(X_test, np.array(y_test)),\n","                    callbacks=pretrain_callbacks,\n","                    verbose=2,\n","                    shuffle=True)"],"metadata":{"id":"bgaNPEJgUC8u"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bd6740eeb4454a14b917b4e7930ff106":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7e965d0b76c426ca2c3acc83c902d8d","IPY_MODEL_ce099dd22061425cb11605de93fa2ec8","IPY_MODEL_4b891624330647bb8edefdc8fa8d5f31"],"layout":"IPY_MODEL_3e49bcab159f4df0bcec1eb8df399b7b"}},"b7e965d0b76c426ca2c3acc83c902d8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2cd225182ff45b49bc7280c23a810d6","placeholder":"​","style":"IPY_MODEL_31dceaa5d3cb4d4dbbde686f966535ef","value":"  0%"}},"ce099dd22061425cb11605de93fa2ec8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bc8c7037021485284325e09f71f69de","max":1994,"min":0,"orientation":"horizontal","style":"IPY_MODEL_450518c663ea44a9a5d192857b9ec16d","value":0}},"4b891624330647bb8edefdc8fa8d5f31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf30b0e711eb4036aeeda764f1fdbc09","placeholder":"​","style":"IPY_MODEL_a8c3eb1aa2454fce923b631b3fde52ae","value":" 0/1994 [00:00&lt;?, ?it/s]"}},"3e49bcab159f4df0bcec1eb8df399b7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2cd225182ff45b49bc7280c23a810d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31dceaa5d3cb4d4dbbde686f966535ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2bc8c7037021485284325e09f71f69de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"450518c663ea44a9a5d192857b9ec16d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf30b0e711eb4036aeeda764f1fdbc09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8c3eb1aa2454fce923b631b3fde52ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}