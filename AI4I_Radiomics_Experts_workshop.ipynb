{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiGMHG8iYoWV"
   },
   "source": [
    "# Radiomics for experts\n",
    "  \n",
    "<b>'Building a radiomics classifier that distinguishes between benign and malignant lesions will certainly improve clinical handling of suspicious lesions, and minimize the need for invasive procedures' - the aim of all the radiomics researchers.</b>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this script, you will use a privately collected contrast enhanced spectral mammography (CESM) of 1058 patients at Maastricht University Medical Center (MUMC). You will build a classifier to differentiate between malignant and benign breast lesions. Such a model will reduce the necessity for invasive tissue biopsies. If you find this task, or parts of it, too hard, you can \"cheat\" by looking at the beginners notebook, as most tasks are very similar.\n",
    "\n",
    "<b>Note:</b> this dataset is private and allowed to be used only for the purpose of this course! Nevertheless, for this notebook, we provide you with the features extracted from the data as an invert transformation is not possible. If you want to extract the features yourself, you will receive the dedicated instructions at the practical. In this case, please make sure you delete imaging data after the workshop. For this, you can use a precision-medicine-toolbox: https://github.com/primakov/precision-medicine-toolbox. For the hints, you can have a look at the beginners notebook: https://colab.research.google.com/drive/1K-sg8g2Po6Czq5utFS2F6CtetXcrwpIt?usp=sharing.\n",
    "\n",
    "<b>Brief description</b>\n",
    "\n",
    "Mammography is the standard imaging tool for breast-cancer screening, but is prone to false positives and lowered sensitivity in high-density breasts. Contrast enhanced spectral mammography (CESM) depicts breast lesions with more accuracy and with higher sensitivity than regular mammograms. This technique uses an iodine contrast agent in addition to a high energy and low energy mammogram to cancel out background noise and enhance the tumor. The advantage of CESM compared to regular mammography is that breast density does not affect image quality, and it boasts a sensitivity of 93-100%, compared to 75-85% of mammography on regular breasts, and lower than 50% in dense breasts.   \n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1GRt1gWtxPDXGycX6TpDAbH1XAieMQc0t)      \n",
    "A typical contrast-enhanced spectral mammography (CESM) examination (only right mediolateral oblique view shown), consisting of a low-energy (a), high-energy (b) and recombined (c) image. A suspicious lesion is seen on the low-energy image, showing enhancement on the recombined image (white arrows). Histopathology showed invasive ductal carcinoma. The high-energy image is not for diagnostic purposes but is used for construction of the recombined image (Lalji, U.C., Jeukens, C.R.L.P.N., Houben, I. et al. Evaluation of low-energy contrast-enhanced spectral mammography images by comparing them to full-field digital mammography using EUREF image quality criteria. Eur Radiol 25, 2813–2820 (2015). https://doi.org/10.1007/s00330-015-3695-2)\n",
    "\n",
    "<b>Problem</b>  \n",
    "\n",
    "Though CESM improves the detection of breast lesions, there is still a challenge in distinguishing between benign and malignant lesions, as the specificity of CESM is estimated to be 70%. This stands to improve using machine learning (ML) algorithms.\n",
    "\n",
    "<b>Notebook structure</b>  \n",
    "\n",
    "The Python script will take you through the following steps:  \n",
    " - Installing packages and importing libraries,\n",
    " - Reading the data and assigning the outcomes,\n",
    " - Removing redundant features and selecting predictive features,\n",
    " - Creating the classification models.\n",
    "If you still have time, go back and try balancing your data, and fine-tune your models.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "This is an interactive Python notebook. To run it, you don't need to install anything on your PC since the script is executed in the cloud. On the left tab, you can see the 'Files' folder, that contains the data. Results of the script execution will be saved in this folder.  \n",
    "  \n",
    "The information about the curated imaging dataset will be distributed separately. To open DICOM files, you can use 3D Slicer (https://www.slicer.org/), RadiAnt viewer (https://www.radiantviewer.com/), or MicroDicom viewer (https://www.microdicom.com/downloads.html). As feature extraction process takes time, we prepared the Excel tables with the pre-extracted features to work with. All the data will be pulled from the shared Google Drive to the temporary environment of this notebook. Further it is explained how to get these data.\n",
    "\n",
    "Please note, that all the files you pull or upload to this notebook, as well as the files, produced while executing the script, are automatically deleted as soon as you end the session.\n",
    "\n",
    "First of all, the needed Python packages have to be uploaded. Some of them are not installed in the environment of this notebook, so the installation is needed with '!pip install <i>name-of-the-package</i>' command. We recommend you get acquainted with getting documentation and help on these packages. For example, google 'python sklearn' and you will get to the documentation quickly. Importing libraries is a necessary step with most progamming languages, not only Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60834,
     "status": "ok",
     "timestamp": 1687859784089,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "60z4gB0WdAnj",
    "outputId": "03d1ff34-74b3-4a3a-afca-ebaee8f4b023"
   },
   "outputs": [],
   "source": [
    "# installing some packages, which are not part of the present Google Collab environment\n",
    "\n",
    "!pip install precision-medicine-toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwuI_iH4UELF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmtool.AnalysisBox import AnalysisBox\n",
    "from pmtool.ToolBox import ToolBox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueLgxNn08Rgz"
   },
   "source": [
    "Pulling the shared files and unzipping (pay attention at 'Files' tab of this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7226,
     "status": "ok",
     "timestamp": 1687859808252,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "kxo0JwAZ_jh_",
    "outputId": "5484a9b6-5d8c-4a3c-99a2-388aa1cd3239"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1hrVNUSWmqEgLcX2slkENoCzPA9aqKy16\n",
    "!unzip AI4I_Radiomics_Experts_Data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzuYSO_Nmn43"
   },
   "source": [
    "## Feature extraction\n",
    "There are many softwares to perform this step, each with their strengths and weaknesses. Features extraction step takes the longest time, so we are happy to help you with it. We will supply you with \"something we prepared earlier\", the lists of features already extracted with Radiomics software, and outcomes (benign/malignant). The data is already split into train and validation sets. Therefore, you can skip this step or return to it later. If you want to skip this step, please move to 'Reading the data' step.  \n",
    "\n",
    "If you want to extract the features by yourself anyway, you can adapt the code from the beginners notebook: https://colab.research.google.com/drive/1K-sg8g2Po6Czq5utFS2F6CtetXcrwpIt?usp=sharing. Here we perform features extraction with PyRadiomics software (https://pyradiomics.readthedocs.io/en/latest/) with the precision-medicine-toolbox (https://github.com/primakov/precision-medicine-toolbox) interface. These are the open-source Python packages. The key difference is that the present script is using Pyradiomics for feature extraction whereas in the prepared files, Radiomics software (https://radiomics.bio/) was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yja8CdFGvy4b"
   },
   "source": [
    "## Reading the data\n",
    "\n",
    "For this step, you will need to specify the path to the files containing the features and outcomes (in this case 0 or 1 for benign or malignant). It is important that apart from initial dataset cleaning, all the work is only performed in the training dataset and the test set remains untouched.\n",
    "\n",
    "<b>NOTE: the separator character, text quotation, decimal point character change depending on your country settings. If this step fails, add those settings into read_excel function. Also, the outputs from different radiomics softwares can differ wildly, so some thought is necessary when doing this at home.</b>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 10062,
     "status": "ok",
     "timestamp": 1687862854566,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "EQ-YoYDCwJXo",
    "outputId": "ff9342f4-40a5-4be9-b395-e3d71ad4772a"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_excel(\"/content/AI4I_Radiomics_Experts_Data/data_features/WS2022_Experts_training.xlsx\")\n",
    "data_test = pd.read_excel(\"/content/AI4I_Radiomics_Experts_Data/data_features/WS2022_Experts_validation.xlsx\")\n",
    "\n",
    "# let's have a look at our dataframe:\n",
    "data_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FoSexoRwOm8"
   },
   "source": [
    "<b>Getting the features and defining the outcome</b>\n",
    "\n",
    "It's a good time to look at the .csv files using Google Tables or similar software. Ask a helper if you need assistance with this. Let's have a look at the columns and get a lis of the features from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1687859878408,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "-NbbS-JbwePK",
    "outputId": "efabb208-6cee-460f-ad9f-c55c60b9a0af"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for item in data_train.columns:\n",
    "  print (counter, item)\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMkJDNYKA7rc"
   },
   "source": [
    "And get list of the features: print features names from the provided file above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1687859882354,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "dmppgJK-A_g1",
    "outputId": "01089037-624c-4bad-fee1-c1c25027105b"
   },
   "outputs": [],
   "source": [
    "features = list(............)\n",
    "print (len(features))\n",
    "print (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLgipVwkwlj5"
   },
   "source": [
    "Separate the features and the outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1687859884937,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "eRAc1kzqwoQ-",
    "outputId": "37e56f88-5772-411b-cb25-64717ac4ef8c"
   },
   "outputs": [],
   "source": [
    "outcome_train = data_train['.........']\n",
    "outcome_test = data_test['..........']\n",
    "data_train = data_train[........]\n",
    "data_test = data_test[.........]\n",
    "\n",
    "# checking how the features dataframe looks like\n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBm8KYFpyZTW"
   },
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Before building the models, it is always useful to perform the exploratory data analysis to have a look at the data, understand the distribution of the features, and notice some data errors. To perform these steps, we will be using precision-medicine-toolbox (https://github.com/primakov/precision-medicine-toolbox), an open-source Python package, developed in the D-Lab, Maastricht University. If something does not work, look at the error and at the already produced features reports for troubleshooting. Don't hesitate to look into the toolbox documentation to fix the problem: https://precision-medicine-toolbox.readthedocs.io/en/latest/?badge=latest. You can search for available functions to deal with NaN values + the ways to read the data, excluding some features.  \n",
    "  \n",
    "Remember that for now, everything happens in the training data. Time to discuss why this is so important!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12861,
     "status": "ok",
     "timestamp": 1687859901855,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "paW5Ay4lyhi4",
    "outputId": "61f0389e-d5be-40f9-a927-91c9d9964e67"
   },
   "outputs": [],
   "source": [
    "# let's list the parameters of our features dataset\n",
    "parameters = {\n",
    "    'feature_path': \"/content/AI4I_Radiomics_Experts_Data/data_features/WS2022_Experts_training.xlsx\", # path to csv/xls file with features\n",
    "    'outcome_path': \"/content/AI4I_Radiomics_Experts_Data/data_features/WS2022_Experts_training.xlsx\", #path to csv/xls file with outcome (in our case - the same file)\n",
    "    'patient_column': 'LesionID', # name of column with patient ID\n",
    "    'patient_in_outcome_column': 'LesionID', # name of column with patient ID in clinical data file\n",
    "    'outcome_column': 'Outcome', # name of outcome column\n",
    "    'feature_column': features,\n",
    "    'feature_column_to_drop': ['LoG_sigma_0_4_mm_2D_IH_max',\n",
    "                               'LoG_sigma_0_7_mm_2D_IH_max',\n",
    "                               'LoG_sigma_0_4_mm_2D_pos_IH_range',\n",
    "                               'LoG_sigma_0_4_mm_2D_pos_IH_min',\n",
    "                               'LoG_sigma_0_1_mm_2D_pos_IH_max',\n",
    "                               'LoG_sigma_1_mm_2D_IH_max',\n",
    "                               'LoG_sigma_0_7_mm_2D_IH_range',\n",
    "                               'LoG_sigma_1_mm_2D_pos_IH_max',\n",
    "                               'LoG_sigma_0_4_mm_2D_IH_min',\n",
    "                               'LoG_sigma_0_1_mm_2D_pos_IH_range',\n",
    "                               'LoG_sigma_0_1_mm_2D_IH_range',\n",
    "                               'LoG_sigma_1_mm_2D_IH_min',\n",
    "                               'LoG_sigma_0_4_mm_2D_IH_range',\n",
    "                               'LoG_sigma_1_mm_2D_pos_IH_range',\n",
    "                               'voxel_distance',\n",
    "                               'LoG_sigma_0_1_mm_2D_IH_max',\n",
    "                               'LoG_sigma_0_7_mm_2D_pos_IH_max',\n",
    "                               'LoG_sigma_1_mm_2D_IH_range',\n",
    "                               'LoG_sigma_0_4_mm_2D_pos_IH_max',\n",
    "                               'LoG_sigma_0_7_mm_2D_pos_IH_min',\n",
    "                               'LoG_sigma_1_mm_2D_pos_IH_min',\n",
    "                               'LoG_sigma_0_1_mm_2D_pos_IH_min',\n",
    "                               'LoG_sigma_0_7_mm_2D_IH_min',\n",
    "                               'LoG_sigma_0_7_mm_2D_pos_IH_range',\n",
    "                               'LoG_sigma_0_1_mm_2D_IH_min']\n",
    "}\n",
    "\n",
    "# initializing the dataset, containing features\n",
    "fs = AnalysisBox(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1687859933834,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "x_pzrlmcIIIY",
    "outputId": "e78c29f3-387c-4f19-8c87-d4cbe04a4d36"
   },
   "outputs": [],
   "source": [
    "print ('Initial amount of features: ', len(..........))\n",
    "fs.handle_nan(axis=1)\n",
    "print ('Amount of features after exclusion of NaN values: ', len(.............))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_EgaYg4ykq-"
   },
   "source": [
    "Plotting feature distributions to have a visual understanding about feature distributions in classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G37us_8FymlQ"
   },
   "outputs": [],
   "source": [
    "fs............................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvfwDvlnyplK"
   },
   "source": [
    "What do you think about the report?\n",
    "\n",
    "Plotting the correlation matrix to have an idea about how many features are inter-correlated. Why is it important to know about the mutual feature correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igN2PiRyyrNL"
   },
   "outputs": [],
   "source": [
    "fs.............................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzLIHLwNytEG"
   },
   "source": [
    "Performing Mann-Whitney U-test (with False Discovery Rate correction) to see if the feature distributions in classes are statistically different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbmFZWfTywa5"
   },
   "outputs": [],
   "source": [
    "fs................................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idLMYzbVyybP"
   },
   "source": [
    "Plotting univariate ROC curves and calculating AUC scores to have some estimation of the predictive power of every separate feature. Can we just build a classifer based on the best feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRfw0MIay2XS"
   },
   "outputs": [],
   "source": [
    "auc_threshold=0.70\n",
    "fs.plot_univariate_roc(................ , ..............)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce691Exmy8oy"
   },
   "source": [
    "Writing the basic statistics for every feature into .csv file and having a look at the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 30478,
     "status": "ok",
     "timestamp": 1687781071972,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "rEjH5fW9zDaV",
    "outputId": "5a8a9bb1-6c51-468f-afa1-2b5924116ddf"
   },
   "outputs": [],
   "source": [
    "fs.calculate_basic_stats()\n",
    "\n",
    "print('Basic statistics for each feature')\n",
    "pd.read_excel('/content/AI4I_Radiomics_Experts_Data/data_features/WS2022_Experts_training_basic_stats.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjxiXEuFzENo"
   },
   "source": [
    "## Features reduction and selection\n",
    "\n",
    "Do you think we need to change somehow features list, based on the exploratory analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1687860202841,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "8kX0JElHzXn3",
    "outputId": "c0a564d9-7b5c-41f8-a1c5-e1fab5aad026"
   },
   "outputs": [],
   "source": [
    "features_non_nan = data_train[features].dropna(axis=1).columns\n",
    "print ('Number of features without missing values: ', len(...........))\n",
    "\n",
    "features_non_zero_var = data_train[features_non_nan].loc[:,data_train[features_non_nan].std() > 0.3].columns\n",
    "print ('Number of features with non-zero variance: ', len(...............))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5IUCHeFzak0"
   },
   "source": [
    "Removing highly correlated features is a controversial step aimed to reduce the dimensionality of the feature space. Highly correlated features needlessly inflate the dimensionality of feature space. The idea is that highly correlated features can be grouped together and represented by one representative feature.  For features pairs with a high Spearman correlation (r > 0.9) the feature with the highest mean correlation with all remaining features is removed.\n",
    "The opponents to this step argue that just because features are correlated doesn't mean that they don't individually  increase the model's performance.\n",
    "Time to discuss the pro's and con's of this step in more detail!\n",
    "\n",
    "<b>If you have time at the end of today's workshop, the cutoff is certaily a variable that can be played with.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6b4jVyizlb4"
   },
   "outputs": [],
   "source": [
    "def selectNonIntercorrelated(df_in, ftrs, corr_th):\n",
    "\n",
    "    # selection of the features, which are not 'highly intercorrelated' (correlation is defined by Spearman coefficient);\n",
    "    # pairwise correlation between all the features is calculated,\n",
    "    # from each pair of features, considered as intercorrelated,\n",
    "    # feature with maximum sum of all the pairwise Spearman correlation coefficients is a 'candidate to be dropped'\n",
    "    # for stability of the selected features, bootstrapping approach is used:\n",
    "    # in each bootstrap split, the random subsample, stratified in relation to outcome,\n",
    "    # is formed, based on original observations from input dataset;\n",
    "    # in each bootstrap split, 'candidates to be dropped' are detected;\n",
    "    # for each input feature, its frequency to appear as 'candidate to be dropped' is calculated,\n",
    "    # features, appeared in 50 % of splits as 'candidate to be dropped', are excluded from feature set\n",
    "\n",
    "    # input:\n",
    "    # df_in - input dataframe, containing feature values (dataframe, columns = features, rows = observations),\n",
    "    # ftrs - list of dataframe features, used in analysis (list of feature names - string variables),\n",
    "    # corr_th - threshold for Spearman correlation coefficient, defining each pair of features as intercorrelated (float)\n",
    "\n",
    "    # output:\n",
    "    # non_intercorrelated_features - list of names of features, which did not appear as inter-correlated\n",
    "\n",
    "    corr_matrix = df_in.corr(method='spearman').abs()\n",
    "    mean_absolute_corr = corr_matrix.mean()\n",
    "    intercorrelated_features_set = []\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    high_corrs = upper.where(upper > corr_th).dropna(how='all', axis=1).dropna(how='all', axis=0)\n",
    "\n",
    "    for feature in high_corrs.columns:\n",
    "        mean_absolute_main = mean_absolute_corr[feature]\n",
    "        correlated_with_feature = high_corrs[feature].index[pd.notnull(high_corrs[feature])]\n",
    "        for each_correlated_feature in correlated_with_feature:\n",
    "            mean_absolute = mean_absolute_corr[each_correlated_feature]\n",
    "            if mean_absolute_main > mean_absolute:\n",
    "                if feature not in intercorrelated_features_set:\n",
    "                    intercorrelated_features_set.append(feature)\n",
    "            else:\n",
    "                if each_correlated_feature not in intercorrelated_features_set:\n",
    "                    intercorrelated_features_set.append(each_correlated_feature)\n",
    "\n",
    "    non_intercorrelated_features_set = [e for e in ftrs if e not in intercorrelated_features_set]\n",
    "\n",
    "    print ('Non intercorrelated features: ', non_intercorrelated_features_set)\n",
    "\n",
    "    return non_intercorrelated_features_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4knIjgI6zpgf"
   },
   "source": [
    "We will use a threshold of 0.9 for absolute value of Spearmann's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1687860211519,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "sRitM94Czsek",
    "outputId": "786c691a-b347-4495-d7f8-dfa971d004fe"
   },
   "outputs": [],
   "source": [
    "features_non_intercorrelated = selectNonIntercorrelated(.............., ......................, 0.9)\n",
    "print ('Number of non-intercorrelated features: ', len(.....................))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ-vBI2QzuxC"
   },
   "source": [
    "We will perform feature selection using Recursive Feature Elimination (RFE). In this step, feature selection is based on the outcome, so simple models are built and those features that contribute the least to the model are removed recursively. Here you can edit the parameters.\n",
    "\n",
    "<b>You might have heard of variable normalization. Why are we not normalizing the variables (e.g. Z-score normalization)?</b>\n",
    "Hint: e.g. https://stackoverflow.com/questions/57339104/is-normalization-necessary-for-randomforest\n",
    "\n",
    "<b>How many features do we need to select with RFE?\n",
    "Time to discuss pros and cons of using many and a little features.</b>  \n",
    "  \n",
    "There are some rules of thumb on how many features we need in the end:  \n",
    "* $int(\\frac{N_{samples}}{10})$ (Abu-Mostafa, Y. S., Magdon-Ismail, M., & Lin, H. T. (2012). Learning from data (Vol. 4, p. 4). New York: AMLBook.)  \n",
    "* $\\sqrt{N_{samples}}$ (Hua, J., Xiong, Z., Lowey, J., Suh, E., & Dougherty, E. R. (2005). Optimal number of features as a function of sample size for various classification rules. Bioinformatics, 21(8), 1509-1515.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1687860214654,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "8nUlfh620JIt",
    "outputId": "49676dab-c1e2-4abf-9f39-31fed640ca43"
   },
   "outputs": [],
   "source": [
    "print ('Nmber of samples in training dataset: ', ..........................)\n",
    "print ('Number of features to select according to Abu-Mostafa: ',......................... )\n",
    "print ('Number of features to select according to Hua: ', ..............................)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggNVDDIS0j3K"
   },
   "source": [
    "Let's go for the lower number of features since our dataset is not large. Below we implement Recursive Feature Elimination, RFE (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html), based on Random Forest Classifier, RFC (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "Which number of features will you chose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "san1UFHv0k8b"
   },
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(n_estimators=100, random_state=np.random.seed(0))\n",
    "selector = RFE(estimator, n_features_to_select=20, step=1)\n",
    "selector = selector.fit(data_train[features_non_intercorrelated], outcome_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eQ0lfWX0pdA"
   },
   "source": [
    "And then select the features 'supported' by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1687860260076,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "v_XsyJFs0vKe",
    "outputId": "aae59750-8c57-47bd-a7ac-383b3ec531ca"
   },
   "outputs": [],
   "source": [
    "support = selector.get_support()\n",
    "selected_features_set = data_train[features_non_intercorrelated].loc[:, support].columns.tolist()\n",
    "\n",
    "print (selected_features_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sZe7WXF0yuI"
   },
   "source": [
    "<b>How many features give the best performance?</b>\n",
    "\n",
    "Time to discuss why we don't just use 100 features!  \n",
    "  \n",
    "The other option to select a number of features is data-driven and is based on ranking the features and then building classifier adding +1 feature at every step and checking its performance. We can evaluate the performance with some score and plot the dependence of this score from the number of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 41260,
     "status": "ok",
     "timestamp": 1687860306505,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "q5Z6lqOJ2Ii8",
    "outputId": "76ed770b-75ca-4c64-97e4-26a9164f8af3"
   },
   "outputs": [],
   "source": [
    "# ranking the features based on RFE results\n",
    "\n",
    "features_ranks = pd.DataFrame({'Features': features_non_intercorrelated, 'Ranks': selector.ranking_})\n",
    "features_ranks.sort_values(by='Ranks', inplace = True)\n",
    "\n",
    "# taking one best feature first, building the RFC, estimating the performance;\n",
    "# adding +1 next feature, repeating the steps to estimate the performance\n",
    "\n",
    "ftrs_number_tuning = []\n",
    "acc_tuning = []\n",
    "\n",
    "for i in range (1, len(features_ranks)):\n",
    "\n",
    "    ftrs_number_tuning.append(i)\n",
    "    estimator_tuning = RandomForestClassifier(n_estimators=100, random_state=np.random.seed(0))\n",
    "    estimator_tuning.fit(data_train[features_ranks['Features'][:i]], outcome_train)\n",
    "    outcome_pred_tuning = estimator_tuning.predict(data_test[features_ranks['Features'][:i]])\n",
    "    acc_tuning.append(balanced_accuracy_score(outcome_test, outcome_pred_tuning))\n",
    "\n",
    "# plotting the results\n",
    "plt.plot(ftrs_number_tuning, acc_tuning)\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('RFC performance')\n",
    "plt.title('RFC performance depending on number of selected features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-7G-Ks52JVd"
   },
   "source": [
    "What can we conclude from the plot? What are the downsides of the presented implementation? Is it correct to train and evaluate the model on the same samples? Is the selected performance metric correct? What other metrics can we use?\n",
    "\n",
    "## Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw1FY2Z7i5Mc"
   },
   "source": [
    "### MODEL 1: RFC\n",
    "\n",
    "We started with RFC while performing RFE, so let us train this model with the selected features and evaluate in on test data.  \n",
    "\n",
    "Training the model (it's possible to vary the parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1687860350910,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "26P08OHd2Tl_",
    "outputId": "e68443fa-f71f-4837-bcbc-dde5bb2fd86d"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=np.random.seed(0))\n",
    "rfc.fit(...............[selected_features_set], ..............)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKz5JUeC2YAK"
   },
   "source": [
    "Prediction for the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tvi88TlB2aR6"
   },
   "outputs": [],
   "source": [
    "outcome_pred_rfc = rfc.predict(.........................................)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFGcuGAi2c5_"
   },
   "source": [
    "Performance reporting on some key classification scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1687860355205,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "US3QUiD_2l5j",
    "outputId": "022ee524-6978-4b80-a66a-1c324f486c9c"
   },
   "outputs": [],
   "source": [
    "print (classification_report(..............., .................))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxY5UdRS2oA7"
   },
   "source": [
    "Are precision and recall informative metrics? Why don't we report accuracy as a key metric? In which cases accuracy is not suitable scores?  \n",
    "  \n",
    "The other popular metric for classification is Receiver Operating Characteristic (ROC) curve and area under the curve (AUC). We will calculate true positive rates (TPR) and false positive rates (FPR) while varying classification threshold and plot the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1687860358267,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "yZdZn37z2qPQ",
    "outputId": "4adb8471-2bcc-4731-d175-65755b0bcfdf"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(outcome_test, rfc.predict_proba(data_test[selected_features_set])[:, 1])\n",
    "roc_auc = roc_auc_score(...........................................................)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('RFC ROC curve (AUC = {})'.format(roc_auc))\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QneA8cJZ2sP1"
   },
   "source": [
    "In which cases this metric does not give a correct representation of the model performance? Is AUC always the best metric?\n",
    "Most cetainly not! Especially for unbalanced datasets, AUC can be meaningless.\n",
    "http://www.davidsbatista.net/blog/2018/08/19/NLP_Metrics/\n",
    "\"With imbalanced classes, it’s easy to get a high accuracy without actually making useful predictions. So, accuracy as an evaluation metrics makes sense only if the class labels are uniformly distributed\"    \n",
    "\n",
    "To have a better understanding of model behaviour, we will plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1687860361237,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "HzW4Fgn02xoP",
    "outputId": "5d9d0826-e252-4fbe-8978-ddfd0df3747b"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(................., .....................)\n",
    "f = sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix for RFC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YETFriH023pW"
   },
   "source": [
    "\n",
    "\n",
    "### MODEL 2: XGBoost\n",
    "\n",
    "The second model we will build is XGBoost because recently the algorithm was successfull in many machine learning competitions.\n",
    "\n",
    "<b>XGBoost - what is it? It's always best to understand your models. </b>\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_intro.html#\n",
    "\n",
    "Train the model: Here you can chose the max number of boosting iterations, a balance between computing time and accuracy.\n",
    "\n",
    "Presenting the data in the appropriate format for the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc7V2Uxw3EzF"
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data_train[selected_features_set], label=............)\n",
    "dtest = xgb.DMatrix(............[selected_features_set], label=............)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0F-YNfT3G_Q"
   },
   "source": [
    "Defining the parameters: the learning objective is logistic regression for binary classification with probability output, the metric is the area under precision-recall curve (why not ROC AUC?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mt119Hy83Lv-"
   },
   "outputs": [],
   "source": [
    "param = {'objective': 'binary:logistic', 'eval_metric': 'aucpr'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsilJkzb3Ohc"
   },
   "source": [
    "We will train the model on the training set and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5Vdwp5n3Q3k"
   },
   "outputs": [],
   "source": [
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVkWe4n53Sq1"
   },
   "source": [
    "Training the model (number of iterations can be changed here!) and calculating outcomes for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1687860482213,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "JjSYef5k3Vm7",
    "outputId": "a34be5ec-0433-4b5a-c03e-f75ffc3c7e70"
   },
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "\n",
    "outcome_pred_xgb = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsKuk8pI3YPf"
   },
   "source": [
    "Classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1687860484620,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "Kk4JUhKk3bKc",
    "outputId": "0222959e-6fec-48eb-fcf2-9532bba6461c"
   },
   "outputs": [],
   "source": [
    "print (classification_report(............., .................>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXlktROy3c7b"
   },
   "source": [
    "ROC and ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1687860488292,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "2Kg5dlpk3gEd",
    "outputId": "3e1ec2d1-c69d-44d2-a55d-1a8992e0fcb6"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(.............., ..............)\n",
    "roc_auc = roc_auc_score(outcome_test, ................)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('XGBoost ROC curve (AUC = {})'.format(roc_auc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeaigfZW3iXE"
   },
   "source": [
    "Create and display the confusion matrix and derived values for the Xgboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1687860496956,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "HtPhR7Jx33CP",
    "outputId": "7524e29e-4d13-45ec-bbdc-7a8d9c0bf549"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(.............., .............>0.5)\n",
    "f = sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJpHgmIW36hA"
   },
   "source": [
    "<b>Almost done! You will now compare the performance of the models.</b>\n",
    "  \n",
    "Which classifier is better?  \n",
    "\n",
    "To compare ROC AUC scores, we will perform a permuation test for the probabilities obtained on the test set for the both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1602,
     "status": "ok",
     "timestamp": 1687860510996,
     "user": {
      "displayName": "Yousif Widaatalla",
      "userId": "13829451873074884238"
     },
     "user_tz": -120
    },
    "id": "0fu8MAO44AP8",
    "outputId": "35f520ae-a328-4e52-a1a7-a5e8e3495cba"
   },
   "outputs": [],
   "source": [
    "# adapted from https://stackoverflow.com/questions/52373318/how-to-compare-roc-auc-scores-of-different-binary-classifiers\n",
    "#-and-assess-statist\n",
    "\n",
    "def permutation_test_between_clfs(y_test, pred_proba_1, pred_proba_2, nsamples=100):\n",
    "    auc_differences = []\n",
    "    auc1 = roc_auc_score(y_test.ravel(), pred_proba_1.ravel())\n",
    "    auc2 = roc_auc_score(y_test.ravel(), pred_proba_2.ravel())\n",
    "    observed_difference = auc1 - auc2\n",
    "    for _ in range(nsamples):\n",
    "        mask = np.random.randint(2, size=len(pred_proba_1.ravel()))\n",
    "        p1 = np.where(mask, pred_proba_1.ravel(), pred_proba_2.ravel())\n",
    "        p2 = np.where(mask, pred_proba_2.ravel(), pred_proba_1.ravel())\n",
    "        auc1 = roc_auc_score(y_test.ravel(), p1)\n",
    "        auc2 = roc_auc_score(y_test.ravel(), p2)\n",
    "        auc_differences.append(auc1 - auc2)\n",
    "    return observed_difference, np.mean(auc_differences >= observed_difference)\n",
    "\n",
    "print ('Difference, p-value: ',\n",
    "       permutation_test_between_clfs(outcome_test,\n",
    "                                     outcome_pred_xgb,\n",
    "                                     rfc.predict_proba(data_test[selected_features_set])[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmqNYAaL4J8D"
   },
   "source": [
    "After this test, which classifier is better?  \n",
    "  \n",
    "<b>Time for a pre-hackathon!</b> You can go back and \"finetune\" the models (or even implement your models) by changing the parameters you feel comfortable with. See you can increase the model performance. Can you beat the other groups?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
